---
title: "#11: Adobe and Google&#x27;s new video AI tools, Stanford&#x27;s HYPE for GANs, and a conversation with books "
date: 2019-04-14
number: 11
aliases:
  - /issues/11-adobe-and-google-s-new-video-ai-tools-stanford-s-hype-for-gans-and-a-conversation-with-books-170283
---

We‚Äôre back to our regularly scheduled programming in today‚Äôs Dynamically Typed, with lots of news.

In productized AI, there are new AI-enabled video tools in Adobe After Effects and Google Photos, and an article about the art community‚Äôs growing acceptance for AI as an art medium.
In machine learning tech, there‚Äôs Stanford‚Äôs new HYPE metric to measure the quality of generative models, Google‚Äôs disbanding of their AI ethics board, three new ML resources, and much more.
Let‚Äôs dive in.

## Productized Artificial Intelligence üîå

![Adobe After Effects's new content-aware filling for videos. (Adobe)](https://s3.amazonaws.com/revue/items/images/004/475/603/mail/e2c16050fad6bd74f289348df1f820bf.png?1555199811)

_Adobe After Effects's new content-aware filling for videos. (Adobe)_

**Adobe After Effects now has a version of Photoshop‚Äôs content-aware fill feature for video.**
Editors can use any existing tools to crop an unwanted object out of a video (like the cars in the lower screenshot above), and After Effects will then do its best to fill the gaps in a natural-looking way (the empty road in the upper screenshot).
Adobe says the feature is powered by its [Sensei AI platform](https://www.adobe.com/uk/sensei.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) but doesn‚Äôt go into much detail about how its generative model works.
Nonetheless, it‚Äôs a very cool example of productized AI: it‚Äôs built into an existing mainstream tool that doesn‚Äôt require video editors to learn anything about the underlying machine learning, and it‚Äôll save them a lot of manual work in tasks like painting away microphones, removing bystanders in stock footage, etc.
Frederic Lardinois for TechCrunch: [Adobe brings content-aware fill for videos to After Effects](https://techcrunch.com/2019/04/03/adobe-brings-content-aware-fill-for-videos-to-after-effects/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

**Google Photos can now automatically extract ‚Äúspecial moments‚Äù from videos.**
Google annotated clips from a collection public videos with labels like ‚Äúblowing out birthday candle‚Äù and ‚Äúcat wags tail‚Äù and trained a temporal action location network (TALNet) to automatically detect these in videos uploaded to Google Photos.
The app then presents these to users as shareable animations, [such as these example gifs](https://2.bp.blogspot.com/-yURtmQgME0w/XKTohaYt8wI/AAAAAAAAEBI/P_YosDG9M74-z4mDTQYxkC-xtGz2TRw3ACEwYBhgL/s640/image3.gif?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
More:

* Google AI blog: [Capturing Special Video Moments with Google Photos](https://ai.googleblog.com/2019/04/capturing-special-video-moments-with.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)
* Research paper by Yu-Wei Chao et al.: [Rethinking the Faster R-CNN Architecture for Temporal Action Localization](https://arxiv.org/abs/1804.07667?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (arXiv)

**Francesca Gavin wrote about how the art community is embracing‚Äîand paying for‚ÄîAI-generated art.**
She mentions GAN-generated pieces like Obvious‚Äô _Edmond de Belamy_ , which was ([controversially](https://www.theverge.com/2018/10/23/18013190/ai-art-portrait-auction-christies-belamy-obvious-robbie-barrat-gans?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)) auctioned off for $432,500 last year, and Mario Klingemann‚Äôs _Memories of Passersby I_ , which I featured in [DT #9](https://dynamicallytyped.com/issues/9-openai-and-google-s-activation-atlases-a16z-s-ml-startup-investments-and-microsoft-s-ai-pipeline-163609?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
One artwork I hadn‚Äôt heard of before is _Closed Loop_ by Jake Elwes, a two-channel projection of one network that generates an image based on a short description and another network that describes the generated image.
These then go back and forth, describing and visualizing each other‚Äôs outputs.
Elwes has a very mesmerizing video of this [on his website](https://www.jakeelwes.com/project-closedLoop.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
For more AI art, see Gavin‚Äôs article: [Artificial intelligence: the art world‚Äôs weird and wonderful new medium](https://howtospendit.ft.com/art-philanthropy/205746-artificial-intelligence-the-art-world-s-weird-and-wonderful-new-medium?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

## Machine Learning Technology üéõ

![Images generated using StyleGAN, plotted by their HYPE score. (Stanford)](https://s3.amazonaws.com/revue/items/images/004/475/491/mail/7dfa2c3cedf9b2dc4a11b431782be61a.jpeg?1555195140)

_Images generated using StyleGAN, plotted by their HYPE score. (Stanford)_

**Sharon Zhou et al.
at Stanford have proposed HYPE, a new metric for evaluating GANs.**
Generative Adversarial Networks have made some incredible progress in generating realistic-looking images in the last few years (see for example StyleGAN in [DT #4](https://dynamicallytyped.com/issues/4-gan-you-feel-the-love-tonight-151860?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).
One problem, however, is that ‚Äúthere is currently no standardized, validated evaluation‚Äù to objectively compare the quality of images generated by different models.
Zhou proposes Human eYe Perceptual Eveluation, or HYPE (an amazing name for a metric in this space), which is a measure of how quickly human evaluators can tell that a generated image is fake (HYPE-Time), and whether they can tell if it‚Äôs fake at all (HYPE-Infinity).
More:

* Official whitepaper: [HYPE: Human eYe Perceptual Evaluation of Generative Models](https://arxiv.org/abs/1904.01121?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (arXiv)
* Stanford‚Äôs website for HYPE, which mentions that code is coming soon: [hype.stanford.edu](https://hype.stanford.edu/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)

**Wayve demonstrated their self-driving car in an urban environment.**
A few months ago the company showed off an autonomous vehicle that used end-to-end deep learning to translate video inputs directly to driving commands (see [DT #6](https://dynamicallytyped.com/issues/6-deep-reinforcement-learning-from-an-atari-zoo-to-a-self-driving-car-in-20-minutes-155882?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)), driving on a simple rural road.
They‚Äôve refined their models and added satellite navigation as additional input, and they‚Äôre now able to drive on public urban roads in Cambridge, UK.
Their car initially learns policies by imitating human drivers, and then uses reinforcement learning to improve these policies by using safety driver interventions as rewards.
With only 20 hours of non-simulated training, their results are very impressive.
More here:

* Wayve‚Äôs blog post: [Learning to Drive like a Human](https://wayve.ai/blog/driving-like-human?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)
* Mike Butcher for TechCrunch: [Wayve claims ‚Äòworld first‚Äô in driving a car autonomously with only its AI and a SatNav](https://techcrunch.com/2019/04/03/wayve-claims-world-first-in-driving-a-car-autonomously-with-only-its-ai-and-a-satnav/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)

**Li Yuan at the New York Times did a feature on Chinese data labeling companies.**
A lot of modern machine learning is done in a supervised setting, requiring labeling enormous datasets.
In China, where labor is relatively cheap compared to Europe and the United States, ‚Äúdata labeling factories‚Äù like Ruijin Technology Company do exactly that:

> ‚ÄúWe‚Äôre the construction workers in the digital world.
> Our job is to lay one brick after another,‚Äù said Yi Yake, co-founder of a data labeling factory in Jiaxian, a city in central Henan province.
> ‚ÄúBut we play an important role in A.I.
> Without us, they can‚Äôt build the skyscrapers.‚Äù

The companies are popping up in areas outside of large cities where labor and office spaces are cheap, enabling them to pay relatively high wages for the area while still providing labeling services at extremely low prices.
Yuan interviewed some workers at the factories: some find the work too boring, while others prefer it over ‚Äúthe same movement, day after day‚Äù that they‚Äôd otherwise do at an assembly line.
Read the full piece here: [How Cheap Labor Drives China‚Äôs A.I.
Ambitions](https://www.nytimes.com/2018/11/25/business/china-artificial-intelligence-labeling.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

**Google has disbanded its AI ethics board less than two weeks after launch.**
Just a few weeks ago, Google announced that it had formed ATEAC, a council in charge of enforcing the company‚Äôs AI ethics guidelines (see [DT #10](https://dynamicallytyped.com/issues/9-openai-and-google-s-activation-atlases-a16z-s-ml-startup-investments-and-microsoft-s-ai-pipeline-163609?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).
One member of the board, Kay Cole James, is involved with a foundation known for its anti-LGBT views; another, Dyan Gibbens, is CEO of a company that built drones for the US military.
Thousands of Googlers signed a petition against their positions on the board, and two other board members also voiced their protest ([one even publicly resigned](https://twitter.com/ssnstudy/status/1112099054551515138?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).
In response, Google decided to disband ATEAC and to go ‚Äúback to the drawing board.‚Äù James wrote a [Washington Post op-ed about her disappointment at the decision](https://www.washingtonpost.com/opinions/i-wanted-to-help-google-make-ai-more-responsible-instead-i-was-treated-with-hostility/2019/04/09/cafd1fb6-5b07-11e9-842d-7d3ed7eb3957_story.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter&utm_term=.a258f2d619c8), which I think is worth reading, but take it with a grain of salt: people like Kara Swisher think that ATEAC was always always [‚Äúa rubber stamp PR move‚Äù](https://twitter.com/karaswisher/status/1115826067418304512?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and that [James was never qualified](https://twitter.com/karaswisher/status/1115855071756062720?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) to serve on it.
More by Jilian D'Onfro for Forbes: [Google Scraps Its AI Ethics Board Less Than Two Weeks After Launch In The Wake Of Employee Protest](https://www.forbes.com/sites/jilliandonfro/2019/04/04/google-cancels-its-ai-ethics-board-less-than-two-weeks-after-launch-in-the-wake-of-employee-protest/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#1f0a80c76e28).

**Quick ML resource links ‚ö°Ô∏è**

* tf-encrypted is an experimental layer on top of TensorFlow to do machine learning on encrypted data. Links: [code](https://github.com/mortendahl/tf-encrypted?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (GitHub); [paper](https://arxiv.org/abs/1810.08130?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (arXiv)
* Dragonfly is a library for Bayesian optimization, or as they call it, ‚Äútuning hyperparameters without grad students.‚Äù Links: [code](https://github.com/dragonfly/dragonfly?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (GitHub); [paper](https://arxiv.org/abs/1903.06694v1?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (arXiv)
* Dent Reality is a company that does indoor augmented reality positioning and wayfinding without the need for additional hardware; they are now ready to deploy their tech. Links: [Dent Reality](https://www.dentreality.com/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter); [demo](https://www.youtube.com/watch?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter&v=lRQu71VPl2s) (YouTube)

## Tech and Startups üöÄ

**Amazon is planning to launch Project Kuiper, a global satellite-powered internet network.**
Just like the competing initiatives by SpaceX and OneWeb (see [DT #9](https://dynamicallytyped.com/issues/9-openai-and-google-s-activation-atlases-a16z-s-ml-startup-investments-and-microsoft-s-ai-pipeline-163609?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)), Amazon‚Äôs project involves sending a few thousand small satellites into orbit to ‚Äúprovide low-latency, high-speed broadband connectivity to unserved and underserved communities around the world.‚Äù Alan Boyle at GeekWire wrote a great article comparing Amazon‚Äôs project to its competitors: [Amazon to offer broadband access from orbit with 3,236-satellite ‚ÄòProject Kuiper‚Äô constellation](https://www.geekwire.com/2019/amazon-project-kuiper-broadband-satellite/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

## Cool Things ‚ú®

![Talk to Books' response to "Who is Harry Potter's mother?" (Google)](https://s3.amazonaws.com/revue/items/images/004/476/407/mail/f613619bd486760daf9b177ede8b94e8.png?1555238478)

_Talk to Books' response to "Who is Harry Potter's mother?" (Google)_

**Google‚Äôs Talk to Books tool answers your questions using passages from books.**
The tool is trained on human conversation, so when you ask it something like [‚ÄúWho is Harry Potter‚Äôs mother?‚Äù](https://books.google.com/talktobooks/query?q=Who%20is%20Harry%20Potter%27s%20mother%3F&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) or [‚ÄúWhat‚Äôs your favorite scary movie?‚Äù](https://books.google.com/talktobooks/query?q=What%27s%20your%20favorite%20scary%20movie%3F&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) it‚Äôll find passages from books that it thinks would be a natural response to your question.
You can try out the experimental AI at [Talk to Books](https://books.google.com/talktobooks/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

**Thanks for reading!**
As always, let me know what you thought of this issue using the buttons below or by sending me a message.
If you‚Äôre new here, subscribe for a new issue of Dynamically Typed every second Sunday!