---
title: "#46: Snapchat&#x27;s machine learning platform, AI at Apple, and an energy consumption tool for ML research "
date: 2020-08-16
number: 46
aliases:
  - /issues/46-snapchat-s-machine-learning-platform-ai-at-apple-and-an-energy-consumption-tool-for-ml-research-268203
---

Hey everyone, welcome to Dynamically Typed #43!
Today‚Äôs feature story is on productized AI: I wrote about SnapML‚ÄîSnapchat‚Äôs machine learning platform‚Äîand its potential as a platform for creative ML developers to distribute their models to an audience of millions.
Quick links in that section are an in-depth feature on AI at Apple and a new Unscreen release.
Beyond productized AI, I‚Äôve got a blogpost about GPT-3 + AGI and a cool new 3D dataset of London for ML research, plus a training power consumption tool and a new clouds dataset for climate change AI.

_(Also: this is the first time I‚Äôve made a custom header image for a feature story‚Äîright below this, for SnapML.
Let me know what you think!)_

## Productized Artificial Intelligence üîå

![](https://s3.amazonaws.com/revue/items/images/006/383/892/mail/3ad60a6dab191b0421bba5cd9f7577f6.png?1597579662)

**SnapML is a software stack for building Lenses that use machine learning models to interact with the Snapchat camera.**
You can build and train a model in any ONNX-compatible framework (like TensorFlow or PyTorch) and drop it straight into Snapchat‚Äôs [Lens Studio](https://lensstudio.snapchat.com?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) as a SnapML component.
SnapML can then apply some basic preprocessing to the camera feed, run it through the model, and format the outputs in a way that other Lens Studio components can understand.
A segmentation model outputs a video mask, an object detection outputs bounding boxes, a style transfer model outputs a new image, etc.
You even have control over how the model runs: once every frame, in the background, or triggered by a user action.
(More details in the [docs](https://lensstudio.snapchat.com/guides/machine-learning/ml-overview/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).)

Matthew Moelleman has written some great in-depth coverage on SnapML for the [Fritz AI Heartbeat blog](https://heartbeat.fritz.ai?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), including a [technical overview](https://heartbeat.fritz.ai/exploring-snapml-a-technical-overview-45d37114fe81?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and a [walkthrough of making a pizza segmentation Lens](https://heartbeat.fritz.ai/exploring-snapml-working-with-custom-neural-networks-in-lens-studio-57459a51cb3d?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
As he notes, SnapML has the potential to be super interesting as a platform:

> Perhaps most importantly, because these models can be used directly in Snapchat as Lenses, they can quickly become available to millions of users around the world.

Indeed, if you create an ML-powered Snapchat filter in Lens Studio, you can easily [publish and share it using a Snapcode](https://lensstudio.snapchat.com/guides/sharing/sharing-your-lens/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), which users can scan to instantly use the Lens in their snaps.
I don‚Äôt think any other platform has such a streamlined (no-code!) system for distributing trained ML models directly to a user base of this size.
Early SnapML user Hart Woolery, also [speaking to Heartbeat](https://heartbeat.fritz.ai/lens-studio-3-0-introduces-snapml-for-adding-custom-neural-networks-directly-to-snapchat-c2c32ed95b2b?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter):

> It‚Äôs a game-changer.
> At least within the subset of people working on live-video ML models.
> This now becomes the easiest way for ML developers to put their work in front of a large audience.
> I would say it‚Äôs analogous to how YouTube democratized video publishing.
> It also lowers the investment in publishing, which means developers can take increased risks or test more ideas at the same cost.

Similar to YouTube, the first commercial applications of SnapML have been marketing-related: there‚Äôs already a [process for submitting sponsored Lenses](https://lensstudio.snapchat.com/guides/submission/sponsored-lenses/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), which can of course include an ML component.
It‚Äôs not too hard to imagine that some advertising agencies will specialize in building SnapML models that, for example, segment Coke bottles or classify different types of Nike shoes in a Lens.
I bet you can bootstrap a pretty solid company around that pitch.

Another application could be Lenses that track viral challenges: count and display how many pushups someone does, or whether they‚Äôre getting all the steps right for a TikTok dance.
Snapchat is [building many of these things itself](https://www.theverge.com/2020/8/13/21365330/snapchat-augmented-reality-lens-tiktok-dance-dixie-damelio?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), but the open platform leaves lots of room for creative ML engineers to innovate‚Äîand even get a share of this year‚Äôs [$750,000 Official Lens Creators fund](https://adage.com/article/digital/snapchat-commits-750000-ar-influencers/2216611?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
(See some of the creations that came out of the fund [here](https://lensstudio.snapchat.com/news/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).)

The big question for me is whether and how Snapchat will expand these incentives for creating ML-powered Lenses.
The Creators fund tripled in size from 2019 to 2020; will we see it grow again next year?
Or are we going to get an in-app Snapchat store for premium Lenses with revenue sharing for creators?
In any case, I think this will be a very exciting space to follow over the next few years.

**Quick productized AI links üîå**

* üçé Samuel Axon wrote an in-depth feature on [machine learning at Apple](https://arstechnica.com/gadgets/2020/08/apple-explains-how-it-uses-machine-learning-across-ios-and-soon-macos/?utm_brand=ars&utm_campaign=Dynamically%20Typed&utm_medium=social&utm_social-type=owned&utm_source=twitter) for Ars Technica, with input from two executives at the company: John Giannandrea (SVP for ML and AI Strategy) and Bob Borchers (VP of Product Marketing). From handwriting recognition to battery charging optimization, AI has‚Äî[Software 2.0-style](https://medium.com/@karpathy/software-2-0-a64152b37c35?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)‚Äîsteadily been eating its way into more and more of the iOS software stack, far beyond just powering the obvious things like Siri speech recognition and camera roll semantic search. Of course, Giannandrea and Borchers also talk a lot about Apple‚Äôs focus on on-device ML and their ‚Äúneural engine‚Äù accelerator chips. It‚Äôs a long article, but a must-read if you‚Äôre into productized AI.
* üìπ From the folks behind DT-favorite [remove.bg](https://remove.bg?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) ([DT #3](http://ttps//dynamicallytyped.com/issues/3-happy-holidays-149573?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [#5](https://dynamicallytyped.com/issues/5-hey-google-what-s-a-golden-kitty-153366?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [#12](https://dynamicallytyped.com/issues/12-openai-introduces-mozart-to-lady-gaga-and-google-takes-your-best-duck-face-selfies-for-you-173114?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [#16](https://dynamicallytyped.com/issues/16-finding-whales-with-ai-and-97-pages-of-ml-for-climate-change-183400?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)), automatic video background removal tool Unscreen ([#35](https://dynamicallytyped.com/issues/35-completely-automatic-video-background-removal-with-unscreen-and-circuits-for-understanding-neural-networks-230458?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)) [now has a Pro version](https://www.producthunt.com/posts/unscreen-pro?utm_campaign=email-notification&utm_medium=email&utm_source=friend_product_maker) that supports full HD, MP4 export, and unlimited-length videos. It‚Äôs web-only for now but an API is in the works. I‚Äôve really enjoyed following this team‚Äôs progress over the past almost two years, and it‚Äôs great to see they‚Äôre continuing to execute to successfully.

## Machine Learning Research üéõ

* ü§ñ Does GPT-3, OpenAI‚Äôs latest iteration of their gargantuan language model ([DT #42](https://dynamicallytyped.com/issues/42-facial-recognition-exodus-openai-s-new-gpt-3-language-model-and-oil-in-the-cloud-254772?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [#44](https://dynamicallytyped.com/issues/44-one-month-in-gpt-3-powered-openai-api-demos-take-the-web-by-storm-261577?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)) mean we‚Äôre imminently close to artificial general intelligence (AGI) like some of the Twitter hype has been suggesting? Reinforcement learning researcher Julian Togelius says no: in [A very short history of some times we solved AI](https://togelius.blogspot.com/2020/08/a-very-short-history-of-some-times-we.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), he argues that we‚Äôve been moving the goalpost fo AGI for decades. ‚ÄúAlgorithms for search, optimization, and learning that were once causing headlines about how humanity was about to be overtaken by machines are now powering our productivity software. And games, phone apps, and cars. Now that the technology works reliably, it‚Äôs no longer AI (it‚Äôs also a bit boring).‚Äù Forgive the long quotes, but I share Togelius‚Äô views on AGI almost exactly, and he communicates them very succinctly: ‚ÄúSo when will we get to real general artificial intelligence? Probably never. Because we‚Äôre chasing a cloud, which looks solid from a distance but scatters in all directions as we drive into it.‚Äù For his more optimistic conclusion, [read the full blog post](https://togelius.blogspot.com/2020/08/a-very-short-history-of-some-times-we.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
* üèô Cool new dataset by Zhou et al. (2020): [HoliCity: A City-Scale Data Platform for Learning Holistic 3D Structures](https://people.eecs.berkeley.edu/~zyc/holicity/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter). Covering a 20-square-kilometer area of central London, it aligns 6,300 high-resolution panorama photos with a 3D CAD model of the city, ‚Äúwith the ultimate goal of supporting real-world applications including city-scale reconstruction, localization, mapping, and augmented reality.‚Äù The [dataset‚Äôs website](https://people.eecs.berkeley.edu/~zyc/holicity/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) includes a few samples with interactive sliders between their RGB, plane, CAD, and semantic views.

## Artificial Intelligence for the Climate Crisis üåç

* üí° New paper from Henderson et al. (2020): [Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning](https://arxiv.org/abs/2002.05651?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter). The authors propose a framework and [open-source software tool](https://github.com/Breakend/experiment-impact-tracker?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) for measuring the energy use of ML models trained on Linux systems with Intel chips and NVIDIA GPUs. They also use [electricityMap](https://www.electricitymap.org/map?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) to estimate carbon emissions based on energy use, but because datacenters increasingly generate their own solar or wind power, and their electricity‚Äôs carbon intensity may therefore not be the same as that of the grid in their region, I‚Äôd take those numbers with a grain of salt. Anyway, I first came across the tool about half a year ago, and I‚Äôm glad it has picked up some steam since then. Consider [giving it a star on GitHub](https://github.com/Breakend/experiment-impact-tracker?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and using it for your next paper or integrating into your company‚Äôs tooling!
* ‚õÖÔ∏è New dataset from Nielsen et al. (2020): [CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds](https://arxiv.org/abs/2007.07978?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) is ‚Äúthe first publicly available dataset with high-resolution cloud types on a high temporal granularity to the authors‚Äô best knowledge.‚Äù The dataset has over 70k images, each annotated with 10 different cloud types, that were recoded in 15-minute increments from January 1st, 2017 to December 31st, 2018. Sounds like it could be very useful for solar PV nowcasting models ([DT #18](https://dynamicallytyped.com/issues/18-runway-ml-s-app-store-for-ai-google-s-new-youtube-dataset-and-a-trippy-gan-journey-188184?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [#40](https://dynamicallytyped.com/issues/40-pinterest-s-ml-for-board-organization-gan-aided-pixel-art-and-bayesian-optimization-gets-the-distill-treatment-247582?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).

**Thanks for reading!**
As usual, you can let me know what you thought of today‚Äôs issue using the buttons below or by replying to this email.
If you‚Äôre new here, check out the [Dynamically Typed archives](https://dynamicallytyped.com/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) or subscribe below to get a new issues in your inbox every second Sunday.

**If you enjoyed this issue of Dynamically Typed, why not forward it to a friend?**
It‚Äôs by far the best thing you can do to help me grow this newsletter.
ü™ê