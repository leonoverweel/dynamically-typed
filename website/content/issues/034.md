---
title: "#34: Google&#x27;s app for detecting fake news memes, an AI for logical reasoning, and Microsoft&#x27;s library for training trillion-parameter models "
date: 2020-03-01
number: 34
aliases:
  - /issues/34-google-s-app-for-detecting-fake-news-memes-an-ai-for-logical-reasoning-and-microsoft-s-library-for-training-trillion-parameter-models-227577
---

Hey everyone, welcome to Dynamically Typed #34!
Today‚Äôs feature stories include a fake news detection app for journalists by Google and Storyful; Microsoft‚Äôs distributed systems tool for training absolutely massive machine learning models; and a fun logical reasoning demo by the Allen Institute for AI.
On an editorial note: I‚Äôve started drifting into a format of doing one feature story per category, plus a bunch of quick one-to-two-sentence links.
This way I can still share more in-depth thoughts on a few stories, but also cover a lot of other interesting stuff that otherwise wouldn‚Äôt make the cut.
Is this something you all like?
Let me know!

## Productized Artificial Intelligence üîå

**Google and Storyful have launched Source, an app to help journalists fact-check online images and screenshots.**

> Journalists and fact-checkers face huge challenges in sorting accurate information from fast-spreading misinformation.
> But it‚Äôs not just about the words we read.
> Viral images and memes flood our feeds and chats, and often they‚Äôre out-of-context or fake.

Traditionally, journalists and fact checkers often use reverse image search to see if a viral image was real or fake news, but this is difficult because the recency bias in online search means that reprints from the past few hours or days often dominate search results.
The Source app instead finds an image‚Äôs ‚Äúpublic history"‚Äîeven if it has been heavily altered since its original publication‚Äîand highlights whether it has previously shown up on whitelisted or blacklisted domains.
It also automatically extracts and translates text from images, making it easier for journalists to catalogue and search them.
The app is a good example of productized AI because it takes a few machine learning tasks that have been solved to the point of being accessible as simple API calls (in this case: reverse image search, optical character recognition, and machine translation) and wraps them up into an easy-to-use tool for non-expert users.

A while ago I toyed around with the idea of making a tool to extract text from screenshots of tweets, and using that to trace back whether the original tweet still exists (or if it ever did).
I wanted to then hook that up to a twitter bot that you could tag to check whether a screenshot of a tweet was real or fake.
I never got around to building it, partially because of restrictions on Twitter‚Äôs API, but mostly because of privacy concerns (I didn‚Äôt want to enable/encourage [doxxing](https://en.wikipedia.org/wiki/Doxing?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).
But this automatic tweet verifying does seem like it could be very useful for journalists using Source, so I‚Äôm excited to see whether Google and Storyful will add features like this to the app as they develop it further.

You can read more about Source in this story by Irene Jay Liu on Google‚Äôs The Keyword blog: [The new tool helping Asian newsrooms detect fake images.](https://www.blog.google/around-the-globe/google-asia/new-tool-helping-asian-newsrooms-detect-fake-images/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)

**Quick productized AI links üîå**

* ü•¶ Amazon has expanded its Go line of automated shops into its first fully-fledged grocery store. Haddie Golden for The Guardian: [‚ÄòJust walk out‚Äô: Amazon debuts its first supermarket with no checkout lines](https://www.theguardian.com/us-news/2020/feb/25/amazon-go-grocery-supermarket-seattle-technology?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter). How long until all Whole Foods stores (which Amazon owns) work this way? (Thanks for the link, Dad!)
* üß´ A group of scientists has succeeded in using machine learning to find a new E. coli antibiotic. Paper in Cell by Stokes et al. (2020): [A Deep Learning Approach to Antibiotic Discovery](https://www.cell.com/cell/fulltext/S0092-8674\(20\)30102-1?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter); also see [this Andrej Karpathy tweet](https://twitter.com/karpathy/status/1231707127300812800?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
* ü§ñ Facebook has published [a paper](https://arxiv.org/abs/2002.07917?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) about Temporal Interaction EmbeddingS (TIES), a ‚Äúproduction-ready model at Facebook-scale networks‚Äù for ‚Äúcapturing rogue social interactions"‚Äîfor catching networks of bots/fake accounts.

## Machine Learning Research üéõ

![ZeRO's memory usage and the end of one training iteration. (Microsoft Research)](https://s3.amazonaws.com/revue/items/images/005/617/184/mail/49d3dfaf7836b8749203a6d37e684f88.png?1583016448)

_ZeRO's memory usage and the end of one training iteration. (Microsoft Research)_

Rangan Majumder and Junhua Wang wrote about **ZeRO and DeepSpeed, Microsoft Research‚Äôs library for optimizing gargantuan deep learning models.**
This is what they used to train Turing-NLG, which at 17 billion parameters is the largest known language model to date‚Äîcompare that to the 1.5 billion parameters in OpenAI‚Äôs GPT-2 model, which I‚Äôve written about extensively ([DT #8](https://dynamicallytyped.com/issues/8-should-openai-open-source-their-impressive-new-language-model-161119?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [#27](https://dynamicallytyped.com/issues/27-google-s-teachable-machine-2-0-openai-s-gpt-2-xl-and-capturing-solar-energy-with-ai-209719?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).
ZeRO achieves this by partitioning the model that‚Äôs being trained in groups corresponding to sets of layers, with:

* Optimizer state partitioning, where optimizer variables (such as the momentum in Adam) are divided between the memories of different GPUs
* Gradient partitioning, where gradients are divided between GPUs.
* Parameter partitioning, where the model‚Äôs weights are divided between GPUs.

This allows them to do data parallelism (e.g.
having different GPUs train on different subsets of the data) on much larger models than was previously possible, at higher speed and throughput.
The exact process by which this happens is quite complex, but the authors created a nice five-minute animation to explains step by step how a complete training iteration happens in this setting.
For the video and more details, check out the Microsoft Research Blog post, [ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), and [microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) on GitHub.

**Quick ML research + resource links** üéõ ([see all 56 resources](https://www.notion.so/adab36fecaea4306880898f41dcb9cb3?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter&v=cb3a74562c914234ac171931dad6c2e4))

* üß© The Abstraction and Reasoning Corpus (ARC) from Keras creator Fran√ßois Chollet‚Äôs _The Measure of Intelligence_ paper (see [DT #26](https://dynamicallytyped.com/issues/26-chollet-s-measure-of-intelligence-and-bert-in-google-search-207148?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)) now has a [Kaggle competition](https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter): ‚Äúcreate an AI capable of solving reasoning tasks it has never seen before.‚Äù Check out the [discussion posts](https://www.kaggle.com/c/abstraction-and-reasoning-challenge/discussion?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (they‚Äôre great) and let me know if you have ideas for the challenge you‚Äôd like to try out together!
* ‚ö°Ô∏è Microsoft‚Äô open-source repo of best practices, code samples and documentation for computer vision (mostly on Azure): [microsoft/computervision-recipes](https://github.com/microsoft/computervision-recipes?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
* ü§î Yoshua Bengio wants machine learning conferences to start incentivizing Slow Science: [Time to rethink the publication process in machine learning](https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
* üê¶ Plumerai released [Larq Compute Engine](https://docs.larq.dev/compute-engine/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), my coworkers‚Äô new open-source project for deploying binarized neural networks to hardware and for using them for efficient inference. Check out the [launch blog post](https://blog.larq.dev/2020/02/announcing-larq-compute-engine/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (which I helped write!) and help us get the word out by giving [larq/compute-engine](https://github.com/larq/compute-engine?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) a star on GitHub. :)

## Artificial Intelligence for the Climate Crisis üåç

**Quick climate AI links** üåç

* üîå The International Energy Agency (IEA) published a detailed online report on energy use by datacenters; [Data centres and energy ‚Äì from global headlines to local headaches?](https://www.iea.org/commentaries/data-centres-and-energy-from-global-headlines-to-local-headaches?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) In the future, it‚Äôll become increasingly important to schedule machine learning experiments and model training to run at times that datacenters have access to lots of low-carbon electricity (e.g. wind, solar, hydro, nuclear)‚Äîare there tools to do this based on weather predictions and datacenter locations? _Should we build one?_
* ‚ùÑÔ∏è Cool paper by Prabha et al. (2020) on arXiv: [Lake Ice Monitoring with Webcams and Crowd-Sourced Images](https://arxiv.org/abs/2002.07875?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), which introduces both a dataset and state-of-the-art model for observing freezing patterns over time‚Äîwithout having to worry about clouds blocking satellite imaging.

## Cool Things ‚ú®

![ROVER correctly predicts a logical conclusion over the given rules and statement. (Allen Institute for AI)](https://s3.amazonaws.com/revue/items/images/005/618/119/mail/56497bccf0831a453f1bd191f47f5143.png?1583057233)

_ROVER correctly predicts a logical conclusion over the given rules and statement. (Allen Institute for AI)_

**The Allen Institute for AI has an online demo of their logical reasoner called ROVER,** which ‚Äúdetermines whether statements are true or false based on rules given in natural language.‚Äù You can pick from a few pre-made sets of rules or make your own (e.g.
‚ÄúI like marbles.
Marbles are blue.‚Äù) and give ROVER some statements to evaluate based on those rules (‚ÄúI like some blue things.
I like all blue things.‚Äù), and it‚Äôll give you its best guess as to their correctness (‚ÄúTrue‚Äù and ‚ÄúFalse,‚Äù respectively, both with 99% confidence).
ROVER is capable of reasoning over much more complex rule sets, too:

> We find transformers appear to learn rule-based reasoning with high (99%) accuracy on these datasets, and in a way that generalizes to test data requiring substantially deeper chaining than in the training data (95%+ scores).
> We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language.

You can try it for yourself [on the Allen AI website](https://rule-reasoning.apps.allenai.org/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) or check out the paper by Clark et al.
(2020) on arXiv: [Transformers as Soft Reasoners over Language](https://arxiv.org/abs/2002.05867?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

**Quick cool things links ‚ú®**

* üé• Denis Shiryaev upscaled a classic black-and-white movie to 4K and 60 frames per second using a generative adversarial network, and it looks amazing: [Arrival of a Train at La Ciotat (The Lumi√®re Brothers, 1896)](https://www.youtube.com/watch?feature=youtu.be&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter&v=3RYNThid23g). Also see the [original video](https://www.youtube.com/watch?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter&v=MT-70ni4Ddo) and an updated [colorized version](https://www.youtube.com/watch?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter&v=EqbOhqXHL7E) using DeOldify (see [DT #15](https://dynamicallytyped.com/issues/15-neural-avatars-ai-on-the-edge-and-apple-s-new-create-ml-app-180967?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).

**Thanks for reading!**
As usual, you can let me know what you thought of today‚Äôs issue using the buttons below or by replying to this email.
If you‚Äôre new here, check out the [Dynamically Typed archives](https://dynamicallytyped.com/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) or subscribe below to get a new issues in your inbox every second Sunday.

**If you enjoyed this issue of Dynamically Typed, why not forward it to a friend?**
It‚Äôs by far the best thing you can do to help me grow this newsletter.
‚òîÔ∏è