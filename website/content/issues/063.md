---
title: "#63: Three times Distill: Multimodal neurons, branch specialization, and weight banding "
date: 2021-04-11
number: 63
aliases:
  - /issues/63-three-times-distill-multimodal-neurons-branch-specialization-and-weight-banding-501602
---

Hey everyone, welcome to Dynamically Typed #63.
I took today to finally sit down and read three recent articles from [Distill](https://distill.pub/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), the machine learning journal that focuses on AI interpretability with the help of clear writing and great, often interactive, visuals.
(It‚Äôs also my favorite AI publication.) The articles are all quite long, and I‚Äôve done my best to summarize them concisely in the ML Research section below, but ‚Äî as always ‚Äî I highly recommend reading them in full as well.
I‚Äôm also saving the other stories I found for DT over the past two weeks for the next issue, so check back then for more productized AI, ML research, climate AI, and cool stuff!

## Machine Learning Research üéõ

![A CLIP neuron that responds to the concept of Spider-Man ‚Äî in the form of photos, comic drawings, or text.](https://s3.amazonaws.com/revue/items/images/008/769/651/mail/Screen_Shot_2021-04-11_at_12.10.01.png?1618135837)

_A CLIP neuron that responds to the concept of Spider-Man ‚Äî in the form of photos, comic drawings, or text._

* üï∏ _Distill #1:_ [Multimodal Neurons in Artificial Neural Networks](https://distill.pub/2021/multimodal-neurons/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) by Goh et al. (2021), which investigates [CLIP, OpenAI‚Äôs multimodal neural network](https://dynamicallytyped.com/stories/2021/openai-dall-e-clip/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) that learned to match images on the internet to text snippets that surround them. (Probably) unlike older image classification models, CLIP has neurons that ‚Äúlight up‚Äù for high-level concepts that were never explicitly part of any classification dataset. ‚ÄúThese neurons don‚Äôt just select for a single object. They also fire (more weakly) for associated stimuli, such as a Barack Obama neuron firing for Michelle Obama or a morning neuron firing for images of breakfast.‚Äù The article has deep dives into three _neuron families_ : (1) the [Region Neurons](https://distill.pub/2021/multimodal-neurons/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#region-neurons) family (like neurons for the [USA](https://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/862?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) or for [Europe](http://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/218?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter); these links take you to the neurons‚Äô pages on OpenAI Microscope); (2) the [Person Neurons](https://distill.pub/2021/multimodal-neurons/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#person-neurons) family (including [Lady Gaga](http://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/263?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and [Ariana Grande](http://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/2233?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)); and (3) the [Emotion Neurons](https://distill.pub/2021/multimodal-neurons/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#emotion-neurons) family (including [sleepy](http://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/91?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and [happy](http://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/1512?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)). It also highlights a baker‚Äôs dozen other families, from holidays and religions to brands and fictional universes. There‚Äôs even an [LGBTQ+ neuron](https://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/1820?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) that responds to things like rainbow flags and the word ‚Äúpride‚Äù! Beyond this exploration, the article looks at how these abstractions in CLIP can be used: for understanding language, emotion composition, and typographic attacks. The authors also note that ‚ÄúCLIP makes it possible for end-users to ‚Äòroll their own classifier‚Äô by programming the model via intuitive, natural language commands ‚Äî this will likely unlock a broad range of downstream uses of CLIP-style models.‚Äù [Sound familiar](https://dynamicallytyped.com/stories/2020/gpt-3/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)? I wonder how long it‚Äôll take until OpenAI launches a v2 of their API that uses CLIP (+ DALL¬∑E?) for image processing and generation the way v1 uses GPT-3 for text.

![Neurons in different branches of InceptionV1 "specialize" in different kinds of concepts, like all curve-related neurons being grouped in mixed3b_5x5](https://s3.amazonaws.com/revue/items/images/008/770/782/mail/Screen_Shot_2021-04-11_at_13.30.21.png?1618140643)

_Neurons in different branches of InceptionV1 "specialize" in different kinds of concepts, like all curve-related neurons being grouped in mixed3b_5x5_

* ‚õì _Distill #2:_ [Branch Specialization](https://distill.pub/2020/circuits/branch-specialization/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) by Voss et al. (2021), a chapter in the [_Circuits_ thread](https://distill.pub/2020/circuits?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) which includes previous work like [Zoom in on Circuits](https://dynamicallytyped.com/stories/2020/distill-zoom-in-on-circuits/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [Early Vision in CNNs](https://dynamicallytyped.com/stories/2020/distill-early-vision-in-cnns/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [Curve Detectors](https://dynamicallytyped.com/links/ml-research/200621-distill-circuits-curve-detectors/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [Equivariance](https://dynamicallytyped.com/links/ml-research/201220-naturally-occurring-equivariance-in-neural-networks/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [High-Low Frequency Detectors](https://dynamicallytyped.com/links/ml-research/210131-distill-circuits-high-low-frequency-detectors/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), and [Curve Circuits](https://dynamicallytyped.com/links/ml-research/210228-distill-long-reads/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter). In this article, the authors find that similar circuit-level functions tend to group themselves in network branches, which are ‚Äúsequences of layers which temporarily don‚Äôt have access to ‚Äòparallel‚Äô information which is still passed to later layers.‚Äù For example, all 30 curve-related features in InceptionV1‚Äôs _mixed3b_5x5_ layer are concentrated in just one of the layer‚Äôs four branches. The authors hypothesize that this is because of a positive feedback loop during training, where the earlier layer in a branch is incentivized to form low-level features that the later layer uses as primitives for higher-level features. One cool thing about Distill is that it also invites non-AI researchers to provide commentary on articles. In this case, [Matthew Nolan](https://www.ed.ac.uk/discovery-brain-sciences/our-staff/research-groups/matthew-nolan?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and Ian Hawes, neuroscientists at the University of Edinburgh, see a ‚Äústriking parallel‚Äù with the separation of cortical pathways in the human brain.

![Weight banding kind of resembles muscle tissue.](https://s3.amazonaws.com/revue/items/images/008/772/887/mail/Screen_Shot_2021-04-11_at_15.50.20.png?1618149035)

_Weight banding kind of resembles muscle tissue._

* üåà _Distill #3:_ [Weight Banding](https://distill.pub/2020/circuits/weight-banding/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) by Petrov et al. (2021), another chapter in the [_Circuits_ thread](https://distill.pub/2020/circuits/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter). This article explores why weights in some layers display a very distinct banding pattern when visualized using Nonnegative Matrix Factorization (NMF), with the following process: ‚ÄúFor each neuron, we take the weights connecting it to the previous layer. We then use NMF to reduce the number of dimensions corresponding to channels in the previous layer down to 3 factors, which we can map to RGB channels.‚Äù This pattern occurs in the final convolutional layer across InceptionV1, ResNet50, and VGG19 (but not AlexNet, which does not use global average pooling). The authors hypothesize that this horizontal banding pattern ‚Äúis a learned way to preserve [vertical] spatial information as it gets lost through various pooling operations,‚Äù which is enforced by the fact that, in an experiment in which they rotate input images by 90 degrees, the bands also rotate by 90 degrees to become vertical. The article concludes that banding is an example of emergent structure in vision models, but that we can‚Äôt say much about whether this structure is ‚Äúgood‚Äù or ‚Äúbad‚Äù or how its presence should influence architectural decisions; not the most significant conclusions, but a very interesting observation nonetheless.

**Thanks for reading!**
As usual, you can let me know what you thought of today‚Äôs issue using the buttons below or by replying to this email.
If you‚Äôre new here, check out the [Dynamically Typed archives](https://dynamicallytyped.com/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) or subscribe to get a new issues in your inbox every second Sunday.

**If you enjoyed this issue of Dynamically Typed, why not forward it to a friend?**
It‚Äôs by far the best thing you can do to help me grow this newsletter.
ü§ì