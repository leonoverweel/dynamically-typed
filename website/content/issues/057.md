---
title: "#57: DALL¬∑E and CLIP, OpenAI&#x27;s new multimodal neural networks "
date: 2021-01-17
number: 57
aliases:
  - /issues/57-dall-e-and-clip-openai-s-new-multimodal-neural-networks-305089
---

Hey everyone, welcome to Dynamically Typed #57!
In today‚Äôs issue I‚Äôve written about DALL¬∑E and CLIP, two new multimodal neural networks by OpenAI that learn to generate and classify images based on textual prompts in a zero-shot way.
They‚Äôre both very cool so they‚Äôve taken up the bulk of this newsletter, but I‚Äôve also got one quick link each for productized AI, ML research and climate AI, covering the other interesting stuff that happend.

## Productized Artificial Intelligence üîå

* üì∏ Creative Commons photo sharing site [Unsplash](https://unsplash.com/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (where I also [have a profile](https://unsplash.com/@leonoverweel?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)!) has launched a new feature: Visual Search, similar to Google‚Äôs search by image. If you‚Äôve found a photo you‚Äôd like to include in a blog post or presentation, for example, but the image is copyrighted, this new Unsplash feature will help you find similar-looking ones that are free to use. The [launch post](https://unsplash.com/blog/introducing-visual-search/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) doesn‚Äôt go into detail about how Visual Search works, but I‚Äôm guessing some (convolutional) classification model extracts features from all images on Unsplash to create a high-dimensional embedding; the same happens to the image you upload, and the site can then serve you photos that are close together in this embedding space. (Here‚Äôs an example of [how you‚Äôd build that in Keras](https://keras.io/examples/vision/metric_learning/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).)

## Machine Learning Research üéõ

![Two example prompts and resulting generated images from DALL¬∑E](https://s3.amazonaws.com/revue/items/images/007/075/268/mail/Screen_Shot_2021-01-17_at_10.59.57.png?1610877616)

_Two example prompts and resulting generated images from DALL¬∑E_

**OpenAI‚Äôs new ‚Äúmultimodal‚Äù**[ **DALL¬∑E**](https://openai.com/blog/dall-e/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) **and**[ **CLIP**](https://openai.com/blog/clip/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) **models combine text and images,** and also mark the first time that the lab has presented two separate big pieces of work in conjunction.
[In a short blog post](https://openai.com/blog/tags/multimodal/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), which I‚Äôll quote almost in full throughout this story because it also neatly introduces both networks, OpenAI‚Äôs chief scientist [Ilya Sutskever](https://openai.com/blog/authors/ilya?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) explains why:

> A long-term objective of artificial intelligence is to build ‚Äúmultimodal‚Äù neural networks‚ÄîAI systems that learn about concepts in several modalities, primarily the textual and visual domains, in order to better understand the world.
> In our latest research announcements, we present two neural networks that bring us closer to this goal.

These two neural networks are DALL¬∑E and CLIP.
We‚Äôll take a look at them one by one, starting with **DALL¬∑E.**

The name DALL¬∑E is a nod to Salvador Dal√≠, the surrealist artist known for [that painting of melting clocks](https://en.wikipedia.org/wiki/The_Persistence_of_Memory?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), and to WALL¬∑E, the Pixar science-fiction romance about a waste-cleaning robot.
It‚Äôs a bit silly to name an energy-hungry image generation AI after a movie in which lazy humans have fled a polluted Earth to float around in space and do nothing but consume content and food, but given how well the portmanteau works and how cute the WALL¬∑E robots are, I probably would‚Äôve done the same.
Anyway, beyond what‚Äôs in a name, here‚Äôs Sutskever‚Äôs introduction of what DALL¬∑E actually does:

> The first neural network, DALL¬∑E, can successfully turn text into an appropriate image for a wide range of concepts expressible in natural language.
> DALL¬∑E uses the same approach used for GPT-3, in this case applied to text‚Äìimage pairs represented as sequences of ‚Äútokens‚Äù from a certain alphabet.

DALL¬∑E builds on two previous OpenAI models, combining [GPT-3](https://arxiv.org/abs/2005.14165?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)‚Äôs capability to [perform different language tasks without finetuning](https://dynamically-typed.netlify.app/stories/2020/gpt-3/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) with [Image GPT](https://openai.com/blog/image-gpt/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)‚Äôs capability to generate coherent image completions and samples.
As input it takes a single stream ‚Äî first text tokens for the prompt sentence, then image tokens for the image ‚Äî of up to 1280 tokens, and learns to predict the next token given the previous ones.
Text tokens take the form of [byte-pair encodings](https://en.wikipedia.org/wiki/Byte_pair_encoding?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) of letters, and image tokens are patches from a 32 x 32 grid in the form of latent codes found using a variational autoencoder [similar to VGVAE](https://openai.com/blog/dall-e/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#fn2).
This relatively simple architecture, combined with a large, [carefully designed](https://twitter.com/AlexTamkin/status/1348581947736424448?s=20&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) dataset, gives DALL¬∑E the following laundry list of capabilities, each of which have interactive examples in [OpenAI‚Äôs blog post](https://openai.com/blog/dall-e?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter):

* Controlling attributes
* Drawing multiple objects
* Visualizing perspective and three-dimensionality
* Visualizing internal and external structure (like asking for a macro or x-ray view!)
* Inferring contextual details
* Combining unrelated concepts
* Zero-shot visual reasoning
* Geographic and temporal knowledge

A lot of people from the community have written about DALL¬∑E or played around with its interactive examples.
Some of my favorites include:

* DeepMind researcher Felix‚Äôs Hill‚Äôs [NonCompositional](https://fh295.github.io/noncompositional.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), a blog post on why DALL¬∑E is good at composition without being very systematic (it can draw _a hedgehog-shaped lettuce_ but not _a green cube on a red cube on a blue cube_ )
* K√°roly Zsolnai-Feh√©r‚Äôs [Two Minute Papers video](https://www.youtube.com/watch?feature=youtu.be&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter&v=C7D5EzkhT6A) on DALL¬∑E, OpenAI‚Äôs previous work that led to it, and lots of generation examples
* Fun generations from Twitter and beyond: Janelle Shane‚Äôs [dawnings of DALL-E](https://aiweirdness.com/post/640120026320470016/the-drawings-of-dall-e?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter); Oriol Vinyals‚Äô [soap dispenser in the shape of a glacier](https://twitter.com/OriolVinyalsML/status/1347219207927320581?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter); and Karol Hausman‚Äôs [snail made of a corkscrew](https://twitter.com/hausman_k/status/1346642324172861440?s=12&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

I think DALL¬∑E is the more interesting of the two models, but let‚Äôs also take a quick look at **CLIP**.

![CLIP's performance on different image classification benchmarks.](https://s3.amazonaws.com/revue/items/images/007/075/525/mail/Screen_Shot_2021-01-17_at_13.13.08.png?1610885638)

_CLIP's performance on different image classification benchmarks._

Sutskever:

> CLIP has the ability to reliably perform a staggering set of visual recognition tasks.
> Given a set of categories expressed in language, CLIP can instantly classify an image as belonging to one of these categories in a ‚Äúzero-shot‚Äù way, without the need to fine-tune on data specific to these categories, as is required with standard neural networks.
> Measured against the industry benchmark ImageNet, CLIP outscores the well-known ResNet-50 system and far surpasses ResNet in recognizing unusual images.

Instead of training on a specific benchmark like ImageNet or ObjectNet, CLIP pretrains on a large dataset of text and images scraped from the internet (so without specific human labels for each images).
It performs a proxy training task: ‚Äúgiven an image, predict which out of a set of 32,768 randomly sampled text snippets, was actually paired with it in our dataset.‚Äù To then do actual classification on a benchmark dataset, the labels are transformed to be more descriptive (e.g.
a ‚Äúcat‚Äù label becomes ‚Äúa photo of a cat‚Äù), and CLIP calculates for each label how likely it is to be paired with the image.
It predicts the most likely one to be the label.
As you can see from the image above, this approach is highly effective across datasets.
It‚Äôs also very efficient because, being a zero-shot model, CLIP doesn‚Äôt need to be (re)trained or finetuned for different datasets.

My favorite application so far of CLIP is by [Travis Hoppe](https://twitter.com/metasemantic?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), who used it to [visualize poems using Unsplash photos](https://twitter.com/metasemantic/status/1349446585952989186?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) ‚Äî worth a click!
Another interesting one is actually how it‚Äôs used in combination with DALL¬∑E: after DALL¬∑E generates 512 plausible images for a prompt, CLIP ranks their quality, and only the 32 best ones are returned in the interactive viewer.
Instead of researchers cherry-picking the best results to show in a paper, a different neural net can actually perform this task!

**Quick ML research + resource links** üéõ

![The FCN-8 semantic segmentation network, drawn using PlotNeuralNet.](https://s3.amazonaws.com/revue/items/images/007/074/479/mail/50308846-c2231880-049c-11e9-8763-3daa1024de78.png?1610833310)

_The FCN-8 semantic segmentation network, drawn using PlotNeuralNet._

* ‚ö°Ô∏è [PlotNeuralNet](https://github.com/HarisIqbal88/PlotNeuralNet?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) is an open-source LaTeX package for drawing deep neural networks, also featuring a Python wrapper. I‚Äôve spent hours doing this by hand in Figma in the past, so this is a very welcome change. (Thanks for the tip, Tim!)

_I‚Äôve also collected all 75+ ML research tools previously featured in Dynamically Typed_[ _on a Notion page_](https://www.notion.so/adab36fecaea4306880898f41dcb9cb3?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter&v=cb3a74562c914234ac171931dad6c2e4) _for quick reference.
‚ö°Ô∏è_

## Artificial Intelligence for the Climate Crisis üåç

* üóÉ [Stephen Rasp](https://twitter.com/raspstephan?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) of agriculture climate change adaptation platform [ClimateAi](https://climate.ai?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) has launched [Pangeo ML Datasets](http://mldata.pangeo.io/index.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), a website that collects weather and climate datasets for AI research. It includes both raw datasets and datasets already preprocessed specifically for training machine learning models.

**Thanks for reading!**
As usual, you can let me know what you thought of today‚Äôs issue using the buttons below or by replying to this email.
If you‚Äôre new here, check out the [Dynamically Typed archives](https://dynamicallytyped.com/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) or subscribe below to get a new issues in your inbox every second Sunday.

**If you enjoyed this issue of Dynamically Typed, why not forward it to a friend?**
It‚Äôs by far the best thing you can do to help me grow this newsletter.
‚ùÑÔ∏è