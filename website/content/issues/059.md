---
title: "#59: A visual search engine, Google&#x27;s camera-based vitals measurements, and two AI lab tooling long reads "
date: 2021-02-14
number: 59
aliases:
  - /issues/59-a-visual-search-engine-google-s-camera-based-vitals-measurements-and-two-ai-lab-tooling-long-reads-318314
---

Hey everyone, welcome to Dynamically Typed #59!
I’ve got a classic links issue for you today, featuring:

* **🔌** _Productized AI:_ camera-based vitals measurements on Google Pixel phones; and Microsoft’s new Speller100 model powering input validation in Bing.
* 🎛 _ML Research:_ Papers with Code’s new Datasets section; two tooling long reads from the big AI labs (one from DeepMind on JAX and one from OpenAI on Kubernetes); and a new paper on “the AI brain drain” from academia to industry.
* ✨ _Cool things:_ same.energy, a visual search engine for images that “feel” the same; and a website with submissions to the CVPR/ICCV computer vision art workshop.

Happy reading!
After I send today’s newsletter our, I’ll be heading right back out on the ice — if you haven’t seen it yet, [we’re having a pretty magical winter here in Amsterdam](https://www.instagram.com/p/CLPuKokHIfq/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

PS: Revue, the service I use to send you Dynamically Typed, [got acquired by Twitter](https://www.getrevue.co/profile/the_week_in_newsletters/issues/revue-is-joining-twitter-309265?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
This shouldn’t change anything for you, but I just wanted to let you know.
💌

## Productized Artificial Intelligence 🔌

* ❤️ Google is [adding camera-based vitals measurement to its Fit app](https://blog.google/technology/health/take-pulse-health-and-wellness-your-phone/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) on Android. Initially rolling out to Pixel phones, the new feature can measure your respiratory (breathing) rate by looking at your face and upper torso through the selfie camera — something that, judging from [a cursory Scholar search](https://scholar.google.com/scholar?as_sdt=0%2C5&btnG=&hl=en&q=respiratory%20rate%20computer%20vision&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), was only becoming a mainstream research topic just two years ago! The rate at which computer vision research makes it from an idea to deployment on millions of phones remains pretty astonishing. The Fit app can also read your heart rate when you place your finger on the back-facing camera, though I don’t think this is as new: I’ve used iPhone apps that did this years ago — but one big difference is that Google has actually done clinical studies to validate these features.
* 💱 Jingwen Lu, Jidong Long and Rangan Majumder wrote [a blog post about Speller100](https://www.microsoft.com/en-us/research/blog/speller100-zero-shot-spelling-correction-at-scale-for-100-plus-languages/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), Microsoft’s zero-shot spelling correction models that now collectively work across 100+ languages. Speller100 is currently live in production as part of Microsoft’s Bing search engine, where it corrects typos in search queries — it’s what powers the “did you mean…” prompt. Although this feature has been around for English-language search queries for a very long time, Speller100 newly enables it for a whole host of smaller languages. It’s also an interesting case study of how an AI-powered refinement step of user input can significantly improve a product’s overall experience. By A/B testing Speller100 against not having spelling correction, the researchers found that it reduced the number of pages with no results by 30%, and manual query reformatting by 5%; and that it increased the number of clicks on spelling suggestions by 67%, and clicks on any item on the page by 70%.

## Machine Learning Research 🎛

* 💻 [Papers with Code](https://www.paperswithcode.com?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) is continuing on its quest to index more and more aspects of machine learning research. They’ve now [launched](https://twitter.com/paperswithcode/status/1356627009791807489?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) a new [Datasets section](https://www.paperswithcode.com/datasets?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) that lets you search benchmarks by the dataset and modality they’re based on. The [page for ImageNet](https://www.paperswithcode.com/dataset/imagenet?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), for example, has a description, samples, usage statistics and metadata about the dataset, and links to 52 related benchmarks. Papers with Code’s execution has been impressive lately: in the last year and a half, they’ve also launched [sotabench](https://dynamically-typed.netlify.app/stories/2019/papers-with-code-sotabench/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [Methods](https://dynamically-typed.netlify.app/stories/2020/papers-with-code-methods-knowledge-graph/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [arXiv integration](https://dynamically-typed.netlify.app/links/ml-research/201011-arxiv-and-papers-with-code/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and [Axcell](https://dynamically-typed.netlify.app/links/ml-research/200510-papers-with-code-axcell/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter). These are all useful resources, and it seems their acquisition by Facebook in the middle of it all hasn’t slowed the team down one bit!
* ⚙️ _AI lab tooling long read #1:_ DeepMind published a blog post about [using JAX to accelerate their research](https://deepmind.com/blog/article/using-jax-to-accelerate-our-research?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter). JAX is a modern take on the NumPy API that “includes an extensible system of composable transformation that help support machine learning research” by taking care of differentiation, vectorization (like abstracting batching away from the researcher), and JIT-compilation (for GPUs and TPUs). The Python library now underpins many of DeepMind’s recent publications, and they’ve also open-sourced several components of their internal ecosystem on top of JAX: [Haiku](https://github.com/deepmind/dm-haiku?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [Optax](https://github.com/deepmind/optax?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [RLax](https://github.com/deepmind/rlax?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [Chex](https://github.com/deepmind/chex?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), and [Jraph](https://github.com/deepmind/jraph?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (“it’s pronounced _gif_ ”).
* ⚙️ _AI lab tooling long read #2:_ OpenAI added a blog post about [scaling Kubernetes to 7,500 nodes](https://openai.com/blog/scaling-kubernetes-to-7500-nodes/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter). Kubernetes is a system for orchestrating Docker containers across a datacenter, and I think most compute-heavy companies use it by now. Both startups I’ve worked at also use it for their machine learning workloads — but at a scale on the order of tens or hundreds of nodes, not many thousands. At that scale, a whole load of problems and potential optimizations suddenly become worth their engineer-time to look at, and that’s exactly what OpenAI does in this detailed post. _(A fun fact I quite enjoy and will probably never have a better excuse to share in DT than now:_ Kubernetes is abbreviated as K8s — “K-then-eight-letters-then-s,” like how internationalization is i18n — and there’s a management tool for Kubernetes called [K9s](https://github.com/derailed/k9s?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter). At first sight, the name just looks like a typical programmer move, “K8+1s = K9s,” but it has another level to it: if you pronounce K9s as a word, it sounds like “canines” — dogs! So the logo for K9s is a dog. 🐩)
* 💼 A new paper by [Jurowetzki et al. (2021)](https://arxiv.org/abs/2102.01648?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) quantifies transition flows of machine learning researchers between industry and academia and “finds that researchers working within the field of deep learning as well as those with higher average impact tend to transition into industry.” [Juan Mateos Garcia](https://twitter.com/JMateosGarcia?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), one of the authors, [refers to this as “the AI brain drain.”](https://twitter.com/JMateosGarcia/status/1356915866475069444?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) This is quite a controversial topic and I haven’t made up my mind about how I feel about it yet, so I’d love to hear your thoughts. (Hit that reply button!)

## Cool Things ✨

![Same.Energy visual search](https://s3.amazonaws.com/revue/items/images/007/571/851/mail/Screen_Shot_2021-02-13_at_13.28.51.png?1613219355)

_Same.Energy visual search_

[same.energy](https://same.energy?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) is a deep learning-powered visual search engine built by [Jason Jackson](https://jacobjackson.com/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (the creator of [TabNine](https://jacobjackson.com/first-month/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), see [DT #18](https://dynamically-typed.netlify.app/issues/018/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)), who says it works similarly to [OpenAI’s CLIP](https://dynamically-typed.netlify.app/stories/2021/openai-dall-e-clip/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
It’s a really nice website to just browse around for a while and get lost in, which is also exactly what it’s meant for.
Jackson, on the site’s [about page](https://same.energy/about?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), says:

> We believe that image search should be visual, using only a minimum of words.
> And we believe it should integrate a rich visual understanding, capturing the artistic style and overall mood of an image, not just the objects in it.
> We hope Same Energy will help you discover new styles, and perhaps use them as inspiration.“

This may remind you of Google’s similar image search, but I found that same.energy does a much better job of finding diverse results: where Google will usually show me different crops and resolutions of the same photo or different photos of the same object, same.energy manages to capture the "feel” of an image and show more (different!) images with that same “feel.” Some of my favorite searches so far are __[blue geometric patterns](https://same.energy/search?i=DiiK&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [glass art](https://same.energy/search?i=xHbEn&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), and [rusty surfaces](https://same.energy/search?i=2T5ED&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

**Quick cool things links ✨**

* 🎨 Just like the [NeurIPS ML creativity workshop](https://dynamically-typed.netlify.app/links/cool-things/201220-neurips-2020-ml-creativity-gallery/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) has a gallery of accepted works at [aiartonline.com](http://www.aiartonline.com/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), I found that the [CVPR/ICCV computer vision art workshop](https://sites.google.com/view/ec3v-cvpr2021/computer-vision-art-gallery?authuser=0&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) also has an equivalent: [computervisionart.com](https://computervisionart.com?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)! The winner of the 2019 workshop was Terence Broad, who trained GANs “to have more variation in the colors they produce […] without any data,” and produced an hour-long loop called [(un) stable equilibrium 1:1](https://www.youtube.com/watch?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter&v=r6MB555mXXM). The website also has the short list and all other accepted work, which are worth a browse through. (The [CFP for this year’s workshop](https://twitter.com/elluba/status/1356984409027727360?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) is also now live; the decline is March 15th.)

**Thanks for reading!**
As usual, you can let me know what you thought of today’s issue using the buttons below or by replying to this email.
If you’re new here, check out the [Dynamically Typed archives](https://dynamicallytyped.com/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) or subscribe below to get a new issues in your inbox every second Sunday.

**If you enjoyed this issue of Dynamically Typed, why not forward it to a friend?**
It’s by far the best thing you can do to help me grow this newsletter.
⛸