---
category: ml-research
date: 2020-10-25
emoji: "\U0001F4B1"
issue_number: 51
title: Facebook M2M-100
---

Kyle Wiggers wrote a feature for Venture Beat on [Facebook’s new M2M-100 model](https://venturebeat.com/2020/10/19/facebooks-open-source-m2m-100-model-can-translate-between-100-different-languages/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) — a machine translation model that, unlike e.g.
Google Translate does for many language pairs, does not use English as go-between.
Instead of translating from A to English and then from English to B, it translates form A directly to B — which for 100 languages means there are nearly 10,000 combinations.
The model was trained on 2200 of these combinations, and is a new state of the art (in terms of BLEU) for many non-English language pairs.
The model has 15 billion parameters, continuing the trend that strength really is in numbers for NLP and MT models.
FAIR has open-sourced M2M-100 at [pytorch/fairseq](https://github.com/pytorch/fairseq/tree/master/examples/m2m_100?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).