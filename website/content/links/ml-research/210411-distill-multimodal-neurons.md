---
category: ml-research
date: 2021-04-11
emoji: "\U0001F578"
issue_number: 63
title: 'Distill: Multimodal Neurons'
---

_Distill #1:_ [Multimodal Neurons in Artificial Neural Networks](https://distill.pub/2021/multimodal-neurons/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) by Goh et al.
(2021), which investigates [CLIP, OpenAI’s multimodal neural network](https://dynamicallytyped.com/stories/2021/openai-dall-e-clip/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) that learned to match images on the internet to text snippets that surround them.
(Probably) unlike older image classification models, CLIP has neurons that “light up” for high-level concepts that were never explicitly part of any classification dataset.
“These neurons don’t just select for a single object.
They also fire (more weakly) for associated stimuli, such as a Barack Obama neuron firing for Michelle Obama or a morning neuron firing for images of breakfast.” The article has deep dives into three _neuron families_ : (1) the [Region Neurons](https://distill.pub/2021/multimodal-neurons/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#region-neurons) family (like neurons for the [USA](https://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/862?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) or for [Europe](http://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/218?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter); these links take you to the neurons’ pages on OpenAI Microscope); (2) the [Person Neurons](https://distill.pub/2021/multimodal-neurons/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#person-neurons) family (including [Lady Gaga](http://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/263?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and [Ariana Grande](http://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/2233?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)); and (3) the [Emotion Neurons](https://distill.pub/2021/multimodal-neurons/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#emotion-neurons) family (including [sleepy](http://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/91?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and [happy](http://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/1512?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).
It also highlights a baker’s dozen other families, from holidays and religions to brands and fictional universes.
There’s even an [LGBTQ+ neuron](https://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/1820?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) that responds to things like rainbow flags and the word “pride”!
Beyond this exploration, the article looks at how these abstractions in CLIP can be used: for understanding language, emotion composition, and typographic attacks.
The authors also note that “CLIP makes it possible for end-users to ‘roll their own classifier’ by programming the model via intuitive, natural language commands — this will likely unlock a broad range of downstream uses of CLIP-style models.” [Sound familiar](https://dynamicallytyped.com/stories/2020/gpt-3/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)?
I wonder how long it’ll take until OpenAI launches a v2 of their API that uses CLIP (+ DALL·E?) for image processing and generation the way v1 uses GPT-3 for text.