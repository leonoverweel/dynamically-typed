---
category: ml-research
date: 2021-01-03
emoji: "\U0001F9E0"
issue_number: 56
title: Abstraction and Reasoning at NeurIPS 2020
---

François Chollet’s NeurIPS [talk on abstraction and reasoning](https://slideslive.com/38935790/abstraction-reasoning-in-ai-systems-modern-perspectives?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), based on his 2019 [Measure of Intelligence paper](https://arxiv.org/abs/1911.01547?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (see [DT #26](https://dynamicallytyped.com/issues/26-chollet-s-measure-of-intelligence-and-bert-in-google-search-207148?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)), is a great watch.
It covers the shortcut rule (“you get what you optimize for — at the detriment of everything else”); levels of generalization; and what kinds of abstraction he thinks deep learning can and cannot solve (by itself).
The tutorial also included [a talk on analogical reasoning](https://slideslive.com/38935791/abstraction-reasoning-in-ai-systems-modern-perspectives?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) by Melanie Mitchell and [one on math reasoning](https://slideslive.com/38935792/deep-learning-for-mathematical-reasoning?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) by Christian Szegedy; [this live-tweeted thread](https://twitter.com/tetraduzione/status/1336078387572482048?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) by Antonio Vergari summarizes all three talks in bite-size chunks.
(RE: the previous link, it’d be super cool to see DeepMind try to tackle the ARC challenge — maybe someday.)