---
category: productized-ai
date: 2020-12-06
emoji: "\U0001F3A0"
issue_number: 54
title: Adversarial DeepFakes
---

Another episode in the saga of deepfakes, videos that make real people look like they’re saying or doing things they never said or did.
In the fall of 2019, Facebook, Microsoft, and Google created datasets and challenges to automatically detect deepfakes (see [DT #23](https://dynamicallytyped.com/issues/23-robotic-raspberry-and-lettuce-pickers-2-5-billion-objects-in-pinterest-lens-and-an-analysis-of-the-ai-reproducibility-crisis-199555?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)); in October 2020, Microsoft then launched their Video Authenticator deepfake detection app ([#48](https://dynamicallytyped.com/issues/23-robotic-raspberry-and-lettuce-pickers-2-5-billion-objects-in-pinterest-lens-and-an-analysis-of-the-ai-reproducibility-crisis-199555?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).
Now, just a few months later, [Neekhara et al.
(2020)](https://arxiv.org/abs/2011.09957?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) present an adversarial deepfake model that handily beats those detectors: “We perform our evaluations on the winning entries of the DeepFake Detection Challenge (DFDC) and demonstrate that they can be easily bypassed in a practical attack scenario.” And the carousel goes ‘round.