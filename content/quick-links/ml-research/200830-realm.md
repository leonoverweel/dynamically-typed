---
category: ml-research
date: 2020-08-30
emoji: "\u2753"
issue_number: 47
title: REALM
---

Although increasingly enormous do-it-all language models like [T5](http://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and GPT-3 ([DT #42](https://dynamicallytyped.com/issues/42-facial-recognition-exodus-openai-s-new-gpt-3-language-model-and-oil-in-the-cloud-254772?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [#44](https://dynamicallytyped.com/issues/44-one-month-in-gpt-3-powered-openai-api-demos-take-the-web-by-storm-261577?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)) have been getting a lot of attention ([haha](https://arxiv.org/abs/1706.03762?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)) lately, smaller and more parameter-efficient models are still improving a lot as well.
A recent interesting one is REALM by [Guu et al.
(2020)](https://arxiv.org/abs/2002.08909?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) at Google AI, which, unlike these larger models, separates the encoding of _language_ from the encoding of _knowledge._ Instead of implicitly storing information about the world in the language model’s weights, [it introduces a neural retriever](https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) that learns to find relevant snippets of text from Wikipedia to be fed into the language model as context alongside the original query.
As a result, it achieves a score of 40.4 on [Natural Questions](https://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) with just 300 million parameters, compared to T5’s score of 36.6 with 11 billion parameters—10% better results at 35x fewer parameters.