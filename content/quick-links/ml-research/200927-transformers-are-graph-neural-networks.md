---
category: ml-research
date: 2020-09-27
emoji: "\U0001F310"
issue_number: 49
title: Transformers are graph neural networks
---

Chaitanya K. Joshi wrote an essay for The Gradient where he argues that [Transformers are Graph Neural Networks](https://thegradient.pub/transformers-are-graph-neural-networks/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), equating the former’s attention mechanism to the latter’s aggregation functions.
It’s a great introduction to both model types, and Joshi poses that these two subfields of machine learning can learn a lot from each other.
(Also, he represents nodes in a GNN using emojis instead of letters, and references them as such in the text, which I love.) Great weekend read.