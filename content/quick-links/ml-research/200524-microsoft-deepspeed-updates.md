---
category: ml-research
date: 2020-05-24
emoji: "\U0001F3CE"
issue_number: 40
title: Microsoft DeepSpeed updates
---

[Microsoft released the second version](https://www.microsoft.com/en-us/research/blog/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) of DeepSpeed and its Zero Redundancy Optimizer (ZeRO-2, see [DT #34](https://dynamicallytyped.com/issues/34-google-s-app-for-detecting-fake-news-memes-an-ai-for-logical-reasoning-and-microsoft-s-library-for-training-trillion-parameter-models-227577?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).
These improvements enable training models that are an order of magnitude larger and faster than previously possible: up to 170 billion parameters, at up to 10x previous state-of-the-art speeds.
Itâ€™s open-source on GitHub at [microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).