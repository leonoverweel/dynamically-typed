---
category: ml-research
date: 2020-09-27
emoji: "\U0001F3CE"
issue_number: 49
title: Microsoft's DeepSpeed update
---

Microsoft has updated [DeepSpeed](https://github.com/microsoft/DeepSpeed?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), its open-source library for efficiently training massive ML models (see [DT #34](https://dynamicallytyped.com/issues/34-google-s-app-for-detecting-fake-news-memes-an-ai-for-logical-reasoning-and-microsoft-s-library-for-training-trillion-parameter-models-227577?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [#40](https://dynamicallytyped.com/issues/40-pinterest-s-ml-for-board-organization-gan-aided-pixel-art-and-bayesian-optimization-gets-the-distill-treatment-247582?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)), with [four big improvements](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter): [3D parallelism](http://%20https//www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#toc-heading-0) for training trillion-parameter models; [ZeRO-Offload](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#toc-heading-3) for 10x bigger model training on a single GPU; [Sparse Attention](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#toc-heading-4) kernels for 10x longer input sequences in Transformers; and [1-bit Adam](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#toc-heading-5) for reducing network load in multi-GPU training.
My work focuses on tiny models rather than large ones, so I haven’t gotten a chance to try DeepSpeed, but if any of you have, I’d love to hear about your experience!