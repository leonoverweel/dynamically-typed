---
category: ml-research
date: 2020-09-27
emoji: "👩‍🔬"
issue_number: 49
title: Human skill in training deep networks
---

We’ve seen “tuning hyperparameters without grad students” with [Dragonfly](https://github.com/dragonfly/dragonfly?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) ([DT #11](https://dynamicallytyped.com/issues/11-adobe-and-google-s-new-video-ai-tools-stanford-s-hype-for-gans-and-a-conversation-with-books-170283?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)) but… how much does a researcher’s experience actually correlate with their skills for tuning an ML model?
[Anand et al, (2020)](https://arxiv.org/abs/2008.05981?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) investigated this and found a strong positive correlation between experience and final model accuracy, and “that an experienced participant finds better solutions using fewer resources on average.” Glad to see my skills aren’t yet completely automatable yet!
(The paper is co-authored by [Jan van Gemert](https://jvgemert.github.io?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), who was the first person to explain to me what a convolution is, in a guest lecture during my first year of undergrad.
😊)