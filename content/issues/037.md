---
title: "#37: OpenAI&#x27;s neural network taxonomy, decoding text from brain implants, and models that don&#x27;t exist "
date: 2020-04-12
number: 37
aliases:
  - https://dynamicallytyped.com/issues/37-openai-s-neural-network-taxonomy-decoding-text-from-brain-implants-and-models-that-don-t-exist-236677
---

Hey everyone, welcome to Dynamically Typed #37!
Iâ€™ve pushed the ML research section to the top of todayâ€™s newsletter because OpenAIâ€™s new Distill article is one of the most exciting things Iâ€™ve read in a long time: they investigated the early layers of Googleâ€™s InceptionV1 vision network to an incredible level of detail, resulting in a first-of-its-kind taxonomy of â€œneuron groups.â€ Itâ€™s really cool stuff, so Iâ€™m covering it in depth.

Beyond that, Iâ€™ve got links to neurological work on decoding text from brain implant signals, and to Wayveâ€™s new LIDAR data augmentation tech.
For productized AI, Iâ€™m covering a startup thatâ€™s using GANs to synthesize fake models for ads, as well as links about AR acquisitions and more.
Finally, for cool stuff, I found a paper that generates 2.5D-perspective images based on a single photo with depth information.

## Machine Learning Research ðŸŽ›

![The largest neuron groups in the `mixed3a` layer of InceptionV1. (Olah et al., 2020)](https://s3.amazonaws.com/revue/items/images/005/808/130/mail/1b22ed14889a1b09c65af4e07fce2b66.png?1586603190)

_The largest neuron groups in the `mixed3a` layer of InceptionV1. (Olah et al., 2020)_

**Chris Olah and his OpenAI collaborators published a new Distill article:**[ **An Overview of Early Vision in InceptionV1**](https://distill.pub/2020/circuits/early-vision?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) **.**
This work is part of Distillâ€™s [_Circuits_ thread](https://distill.pub/2020/circuits/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), which aims to understand how convolutional neural networks work by investigating individual features and how they interact through the formation of logical circuits (see [DT #35](https://dynamicallytyped.com/issues/35-completely-automatic-video-background-removal-with-unscreen-and-circuits-for-understanding-neural-networks-230458?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).
In this new article, Olah et al.
explore the first five layers of Googleâ€™s [InceptionV1](https://arxiv.org/abs/1409.4842?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) network:

> Over the course of these layers, we see the network go from raw pixels up to sophisticated [boundary detection](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_boundary), basic shape detection (eg.
> [curves](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_curves), [circles](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_circles_loops), [spirals](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_curve_shapes), [triangles](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3a_angles)), [eye detectors](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_eyes), and even crude detectors for [very small heads](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_proto_head).
> Along the way, we see a variety of interesting intermediate features, including [Complex Gabor detectors](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#conv2d1_discussion_complex_gabor) (similar to some classic â€œcomplex cellsâ€ of neuroscience), [black and white vs color detectors](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#mixed3a_discussion_BW), and [small circle formation from curves](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#mixed3a_discussion_small_circle).

Each of these five layers contains dozens to hundreds of features (a.k.a.
channels or filters) that the authors categorize into human-understandable groups, which consist of features that detect similar things for inputs with slightly different orientations, frequencies, or colors.
This goes from `conv2d0`, the first layer where 85% of filters fall into two simple categories (detectors [for lines](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_conv2d0_gabor_filters) and [for contrasting colors](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_conv2d0_color_contrast), in various orientations), all the way up to `mixed3b`, the fifth layer where there are over a dozen complex categories (detectors [for small heads](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_proto_head), [for circles/loops](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_circles_loops), and much more).
Weâ€™ve known that there are line detectors in early network layers for a long time, but this detailed taxonomy of later-layer features is novelâ€”and it mustâ€™ve been an enormous amount of work to create.

![A cicuits-based visualization of the black & white detector neuron group in layer `mixed3a` of InceptionV1. (Olah et al., 2020)](https://s3.amazonaws.com/revue/items/images/005/808/297/mail/359e82fcba2c004daa134d52da10992b.png?1586611023)

_A cicuits-based visualization of the black & white detector neuron group in layer `mixed3a` of InceptionV1. (Olah et al., 2020)_

For a few of the categories, like black & white and small circle detectors in `mixed3a`, and boundary and fur detectors in `mixed3b`, the article also investigates the â€œcircuitsâ€ that formed them.
Such circuits show how strongly the presence of a feature in the input positively or negatively influences (â€œexcitesâ€ or â€œinhibitsâ€) different regions of the current feature.
One of the most interesting aspects of this research is that some of these circuitsâ€”which were learned by the network, not explicitly programmed!â€”are super intuitive once you think about them for a bit.
The black & white detector above, for example, consists mostly of negative weights that inhibit colorful input features: the more color features in the input, the less likely it is to be black & white.

The simplicity of many of these circuits suggests, to me at least, that Olah et al.
are currently exploring one of the most promising paths in AI explainability research.
(Although there is an alternate possibility, as pointed out by the authors: that theyâ€™ve found a â€œtaxonomy that might be helpful to humans but [that] is ultimately somewhat arbitrary.â€)

Anyway, [An Overview of Early Vision in InceptionV1](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_proto_head) is one of the most fascinating machine learning papers Iâ€™ve read in a long time, and I spent a solid hour zooming in on different parts of the taxonomy.
[The groups for layer `mixed3a`](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#mixed3a) are probably my favorite.
Iâ€™m also curious about how much these early-layer neuron groups generalize to other vision architectures and types of networksâ€”to what extent, for example, do these same neuron categories show up in the first layers of binarized neural networks?

If you read the article and have more thoughts about it that I didnâ€™t cover here, Iâ€™d love to hear them.
:)

**Quick ML research + resource links** ðŸŽ› ([see all 59 resources](https://www.notion.so/adab36fecaea4306880898f41dcb9cb3?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter&v=cb3a74562c914234ac171931dad6c2e4))

* ðŸ§  Neurologists Joseph Makin et al. at UC San Francisco used a-250 electrode brain implant to [decode human brain signals into text](https://www.biorxiv.org/content/10.1101/708206v1?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) with techniques from machine translationâ€”at a word error rate of only 3%. The implant technology [wonâ€™t be widely usable anytime soon](https://news.ycombinator.com/item?id=22736681&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), if ever, but you can download the code it runs on anyway: [jgmakin/machine_learning](https://github.com/jgmakin/machine_learning?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
* ðŸš˜ Shuyang Cheng et al. at self-driving car company Waymo have extended Googleâ€™s reinforcement-learned image data augmentation technique, [AutoAugment](https://ai.googleblog.com/2018/06/improving-deep-learning-performance.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), to work with [LIDAR data](https://blog.waymo.com/2020/04/using-automated-data-augmentation-to.html?m=1&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter). Elon Musk still believes LIDAR sensors are [useless â€œappendicesâ€](https://techcrunch.com/2019/04/22/anyone-relying-on-lidar-is-doomed-elon-musk-says/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) for self-driving cars but the rest of the industry, Waymo included, is evidently not getting anywhere closer to agreeing with that thesis.

## Productized Artificial Intelligence ðŸ”Œ

![None of these models exist. (Rosebud AI)](https://s3.amazonaws.com/revue/items/images/005/808/816/mail/bc0f1b0878c25d143962b650ac0ba28e.png?1586616062)

_None of these models exist. (Rosebud AI)_

[**Rosebud AI**](https://www.generative.photos/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) **uses generative adversarial networks (GANs) to synthesize photos of fake people for ads.**
Weâ€™ve of course seen a lot of GAN face generation in the past (see [DT #6](https://dynamicallytyped.com/issues/6-deep-reinforcement-learning-from-an-atari-zoo-to-a-self-driving-car-in-20-minutes-155882?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [#8](https://dynamicallytyped.com/issues/8-should-openai-open-source-their-impressive-new-language-model-161119?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [#23](https://dynamicallytyped.com/issues/23-robotic-raspberry-and-lettuce-pickers-2-5-billion-objects-in-pinterest-lens-and-an-analysis-of-the-ai-reproducibility-crisis-199555?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)), but this is one of the first startups Iâ€™ve come across thatâ€™s building a product around it.
Their pitch to advertisers is simple: take photos from your previous photoshoots, and weâ€™ll automatically swap out the modelâ€™s face with one better suited to the demographic youâ€™re targeting.
The new face can either be GAN-generated or licensed from real models on the [generative.photos platform](https://www.generative.photos/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
But either way, Rosebud AIâ€™s software takes care of inserting the face in a natural-looking way.

This raises some obvious questions: is it OK to advertise using nonexistent people?
Do you need modelsâ€™ explicit consent to reuse their body with a new face?
How does copyright work when your model is half real, half generated?
Iâ€™m sure Rosebud AIâ€™s founders spend a lot of time thinking about these questions; and as they do, you can follow their along with their thoughts on [Twitter](https://twitter.com/Rosebud_AI?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and [Instagram](https://www.instagram.com/generative.photos/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

**Quick productized AI links ðŸ”Œ**

* ðŸ““ I recently came across Martin Zinkevichâ€™s 24-page [Rules of Machine Learning](http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (PDF), â€œintended to help those with a basic knowledge of machine learning get the benefit of best practices in machine learning from around Google.â€ Lots of good stuff in here.
* ðŸ’¸ Acquisitions in the augmented reality space are heating up: in the past two weeks alone, [Ikea bought Geomagical Labs](https://techcrunch.com/2020/04/02/ikea-acquires-ai-imaging-startup-geomagical-labs-to-supercharge-room-visualisations/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (which works on placing virtual furniture in rooms; an obvious fit) and PokÃ©mon Go developer [Niantic acquired 6D.ai](https://techcrunch.com/2020/03/31/niantic-acquires-ar-startup-6d-ai-as-the-game-creator-squares-up-against-apple-facebook/?utm_campaign=409fe0dda4-EMAIL_CAMPAIGN_2019_04_24_03_18_COPY_01&utm_medium=email&utm_source=Deep%20Learning%20Weekly&utm_term=0_384567b42d-409fe0dda4-157030505) (which works on indoor mapping; another obvious fit).
* ðŸ“¸ Cool new paper from Liu et al. (2020): [Learning to See Through Obstructions](https://arxiv.org/abs/2004.01180?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) proposes a learning-based approach that can remove things like chain fences and window reflections from photos (see the [paper PDF ](https://arxiv.org/pdf/2004.01180.pdf?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)for examples). This isnâ€™t yet productized, but of the collaborators works at Googleâ€”so: how long until this shows up up in the camera app for Pixel phones?

## Cool Things âœ¨

![Layered depth inpainting. (Shih et al., 2020)](https://s3.amazonaws.com/revue/items/images/005/809/755/mail/aeb49373886c4e9513730e682e292d7c.png?1586634824)

_Layered depth inpainting. (Shih et al., 2020)_

Hereâ€™s another cool AI art piece that canâ€™t be done justice using just the static screenshot above: Shih et al.
(2020) published [**3D Photography using Context-aware Layered Depth Inpainting**](https://shihmengli.github.io/3D-Photo-Inpainting/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) at this yearâ€™s CVPR conference.
Hereâ€™s what that means:

> We propose a method for converting a single RGB-D input image into a 3D photo, i.e., a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view.

Based on a single image (plus depth information), they can generate a 2.5-dimensional representation, realistically re-rendering the scene from slightly different perspectives from which it was originally taken.
Contrast that with recent work on neural radiance fields, which requires on the order of 20 - 50 images to work (see [DT #36](https://dynamicallytyped.com/issues/36-google-releases-tensorflow-quantum-software-2-0-at-plumerai-and-encoding-scenes-in-neural-networks-233576?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).

Shih et al.
set up [a website with some fancy demos](https://shihmengli.github.io/3D-Photo-Inpainting/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), which is definitely worth a look; see [these gifs on Twitter too](https://twitter.com/genekogan/status/1248650281249673217?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
One of the authors also works at Facebook, so I wonder if weâ€™ll one day see Instagram filters with this effectâ€”or if itâ€™ll be a part of Facebookâ€™s virtual reality ambitions.
Since the next generation of iPhones will likely have a depth sensor on the back too, I expect weâ€™ll see a lot of this 2.5D photography stuff in the coming years.

**Thanks for reading!**
As usual, you can let me know what you thought of todayâ€™s issue using the buttons below or by replying to this email.
If youâ€™re new here, check out the [Dynamically Typed archives](https://dynamicallytyped.com/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) or subscribe below to get a new issues in your inbox every second Sunday.

**If you enjoyed this issue of Dynamically Typed, why not forward it to a friend?**
Itâ€™s by far the best thing you can do to help me grow this newsletter.
ðŸŒž