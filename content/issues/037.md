---
title: "#37: OpenAI&#x27;s neural network taxonomy, decoding text from brain implants, and models that don&#x27;t exist "
date: 2020-04-12
number: 37
aliases:
  - https://dynamicallytyped.com/issues/37-openai-s-neural-network-taxonomy-decoding-text-from-brain-implants-and-models-that-don-t-exist-236677
---

Hey everyone, welcome to Dynamically Typed #37!
I‚Äôve pushed the ML research section to the top of today‚Äôs newsletter because OpenAI‚Äôs new Distill article is one of the most exciting things I‚Äôve read in a long time: they investigated the early layers of Google‚Äôs InceptionV1 vision network to an incredible level of detail, resulting in a first-of-its-kind taxonomy of ‚Äúneuron groups.‚Äù It‚Äôs really cool stuff, so I‚Äôm covering it in depth.

Beyond that, I‚Äôve got links to neurological work on decoding text from brain implant signals, and to Wayve‚Äôs new LIDAR data augmentation tech.
For productized AI, I‚Äôm covering a startup that‚Äôs using GANs to synthesize fake models for ads, as well as links about AR acquisitions and more.
Finally, for cool stuff, I found a paper that generates 2.5D-perspective images based on a single photo with depth information.

## Machine Learning Research üéõ

![The largest neuron groups in the `mixed3a` layer of InceptionV1. (Olah et al., 2020)](https://s3.amazonaws.com/revue/items/images/005/808/130/mail/1b22ed14889a1b09c65af4e07fce2b66.png?1586603190)

_The largest neuron groups in the `mixed3a` layer of InceptionV1. (Olah et al., 2020)_

**Chris Olah and his OpenAI collaborators published a new Distill article:**[ **An Overview of Early Vision in InceptionV1**](https://distill.pub/2020/circuits/early-vision?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) **.**
This work is part of Distill‚Äôs [_Circuits_ thread](https://distill.pub/2020/circuits/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), which aims to understand how convolutional neural networks work by investigating individual features and how they interact through the formation of logical circuits (see [DT #35](https://dynamicallytyped.com/issues/35-completely-automatic-video-background-removal-with-unscreen-and-circuits-for-understanding-neural-networks-230458?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).
In this new article, Olah et al.
explore the first five layers of Google‚Äôs [InceptionV1](https://arxiv.org/abs/1409.4842?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) network:

> Over the course of these layers, we see the network go from raw pixels up to sophisticated [boundary detection](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_boundary), basic shape detection (eg.
> [curves](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_curves), [circles](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_circles_loops), [spirals](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_curve_shapes), [triangles](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3a_angles)), [eye detectors](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_eyes), and even crude detectors for [very small heads](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_proto_head).
> Along the way, we see a variety of interesting intermediate features, including [Complex Gabor detectors](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#conv2d1_discussion_complex_gabor) (similar to some classic ‚Äúcomplex cells‚Äù of neuroscience), [black and white vs color detectors](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#mixed3a_discussion_BW), and [small circle formation from curves](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#mixed3a_discussion_small_circle).

Each of these five layers contains dozens to hundreds of features (a.k.a.
channels or filters) that the authors categorize into human-understandable groups, which consist of features that detect similar things for inputs with slightly different orientations, frequencies, or colors.
This goes from `conv2d0`, the first layer where 85% of filters fall into two simple categories (detectors [for lines](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_conv2d0_gabor_filters) and [for contrasting colors](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_conv2d0_color_contrast), in various orientations), all the way up to `mixed3b`, the fifth layer where there are over a dozen complex categories (detectors [for small heads](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_proto_head), [for circles/loops](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_circles_loops), and much more).
We‚Äôve known that there are line detectors in early network layers for a long time, but this detailed taxonomy of later-layer features is novel‚Äîand it must‚Äôve been an enormous amount of work to create.

![A cicuits-based visualization of the black & white detector neuron group in layer `mixed3a` of InceptionV1. (Olah et al., 2020)](https://s3.amazonaws.com/revue/items/images/005/808/297/mail/359e82fcba2c004daa134d52da10992b.png?1586611023)

_A cicuits-based visualization of the black & white detector neuron group in layer `mixed3a` of InceptionV1. (Olah et al., 2020)_

For a few of the categories, like black & white and small circle detectors in `mixed3a`, and boundary and fur detectors in `mixed3b`, the article also investigates the ‚Äúcircuits‚Äù that formed them.
Such circuits show how strongly the presence of a feature in the input positively or negatively influences (‚Äúexcites‚Äù or ‚Äúinhibits‚Äù) different regions of the current feature.
One of the most interesting aspects of this research is that some of these circuits‚Äîwhich were learned by the network, not explicitly programmed!‚Äîare super intuitive once you think about them for a bit.
The black & white detector above, for example, consists mostly of negative weights that inhibit colorful input features: the more color features in the input, the less likely it is to be black & white.

The simplicity of many of these circuits suggests, to me at least, that Olah et al.
are currently exploring one of the most promising paths in AI explainability research.
(Although there is an alternate possibility, as pointed out by the authors: that they‚Äôve found a ‚Äútaxonomy that might be helpful to humans but [that] is ultimately somewhat arbitrary.‚Äù)

Anyway, [An Overview of Early Vision in InceptionV1](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#group_mixed3b_proto_head) is one of the most fascinating machine learning papers I‚Äôve read in a long time, and I spent a solid hour zooming in on different parts of the taxonomy.
[The groups for layer `mixed3a`](https://distill.pub/2020/circuits/early-vision/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#mixed3a) are probably my favorite.
I‚Äôm also curious about how much these early-layer neuron groups generalize to other vision architectures and types of networks‚Äîto what extent, for example, do these same neuron categories show up in the first layers of binarized neural networks?

If you read the article and have more thoughts about it that I didn‚Äôt cover here, I‚Äôd love to hear them.
:)

**Quick ML research + resource links** üéõ ([see all 59 resources](https://www.notion.so/adab36fecaea4306880898f41dcb9cb3?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter&v=cb3a74562c914234ac171931dad6c2e4))

* üß† Neurologists Joseph Makin et al. at UC San Francisco used a-250 electrode brain implant to [decode human brain signals into text](https://www.biorxiv.org/content/10.1101/708206v1?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) with techniques from machine translation‚Äîat a word error rate of only 3%. The implant technology [won‚Äôt be widely usable anytime soon](https://news.ycombinator.com/item?id=22736681&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), if ever, but you can download the code it runs on anyway: [jgmakin/machine_learning](https://github.com/jgmakin/machine_learning?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
* üöò Shuyang Cheng et al. at self-driving car company Waymo have extended Google‚Äôs reinforcement-learned image data augmentation technique, [AutoAugment](https://ai.googleblog.com/2018/06/improving-deep-learning-performance.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), to work with [LIDAR data](https://blog.waymo.com/2020/04/using-automated-data-augmentation-to.html?m=1&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter). Elon Musk still believes LIDAR sensors are [useless ‚Äúappendices‚Äù](https://techcrunch.com/2019/04/22/anyone-relying-on-lidar-is-doomed-elon-musk-says/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) for self-driving cars but the rest of the industry, Waymo included, is evidently not getting anywhere closer to agreeing with that thesis.

## Productized Artificial Intelligence üîå

![None of these models exist. (Rosebud AI)](https://s3.amazonaws.com/revue/items/images/005/808/816/mail/bc0f1b0878c25d143962b650ac0ba28e.png?1586616062)

_None of these models exist. (Rosebud AI)_

[**Rosebud AI**](https://www.generative.photos/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) **uses generative adversarial networks (GANs) to synthesize photos of fake people for ads.**
We‚Äôve of course seen a lot of GAN face generation in the past (see [DT #6](https://dynamicallytyped.com/issues/6-deep-reinforcement-learning-from-an-atari-zoo-to-a-self-driving-car-in-20-minutes-155882?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [#8](https://dynamicallytyped.com/issues/8-should-openai-open-source-their-impressive-new-language-model-161119?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), [#23](https://dynamicallytyped.com/issues/23-robotic-raspberry-and-lettuce-pickers-2-5-billion-objects-in-pinterest-lens-and-an-analysis-of-the-ai-reproducibility-crisis-199555?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)), but this is one of the first startups I‚Äôve come across that‚Äôs building a product around it.
Their pitch to advertisers is simple: take photos from your previous photoshoots, and we‚Äôll automatically swap out the model‚Äôs face with one better suited to the demographic you‚Äôre targeting.
The new face can either be GAN-generated or licensed from real models on the [generative.photos platform](https://www.generative.photos/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
But either way, Rosebud AI‚Äôs software takes care of inserting the face in a natural-looking way.

This raises some obvious questions: is it OK to advertise using nonexistent people?
Do you need models‚Äô explicit consent to reuse their body with a new face?
How does copyright work when your model is half real, half generated?
I‚Äôm sure Rosebud AI‚Äôs founders spend a lot of time thinking about these questions; and as they do, you can follow their along with their thoughts on [Twitter](https://twitter.com/Rosebud_AI?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and [Instagram](https://www.instagram.com/generative.photos/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

**Quick productized AI links üîå**

* üìì I recently came across Martin Zinkevich‚Äôs 24-page [Rules of Machine Learning](http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (PDF), ‚Äúintended to help those with a basic knowledge of machine learning get the benefit of best practices in machine learning from around Google.‚Äù Lots of good stuff in here.
* üí∏ Acquisitions in the augmented reality space are heating up: in the past two weeks alone, [Ikea bought Geomagical Labs](https://techcrunch.com/2020/04/02/ikea-acquires-ai-imaging-startup-geomagical-labs-to-supercharge-room-visualisations/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (which works on placing virtual furniture in rooms; an obvious fit) and Pok√©mon Go developer [Niantic acquired 6D.ai](https://techcrunch.com/2020/03/31/niantic-acquires-ar-startup-6d-ai-as-the-game-creator-squares-up-against-apple-facebook/?utm_campaign=409fe0dda4-EMAIL_CAMPAIGN_2019_04_24_03_18_COPY_01&utm_medium=email&utm_source=Deep%20Learning%20Weekly&utm_term=0_384567b42d-409fe0dda4-157030505) (which works on indoor mapping; another obvious fit).
* üì∏ Cool new paper from Liu et al. (2020): [Learning to See Through Obstructions](https://arxiv.org/abs/2004.01180?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) proposes a learning-based approach that can remove things like chain fences and window reflections from photos (see the [paper PDF ](https://arxiv.org/pdf/2004.01180.pdf?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)for examples). This isn‚Äôt yet productized, but of the collaborators works at Google‚Äîso: how long until this shows up up in the camera app for Pixel phones?

## Cool Things ‚ú®

![Layered depth inpainting. (Shih et al., 2020)](https://s3.amazonaws.com/revue/items/images/005/809/755/mail/aeb49373886c4e9513730e682e292d7c.png?1586634824)

_Layered depth inpainting. (Shih et al., 2020)_

Here‚Äôs another cool AI art piece that can‚Äôt be done justice using just the static screenshot above: Shih et al.
(2020) published [**3D Photography using Context-aware Layered Depth Inpainting**](https://shihmengli.github.io/3D-Photo-Inpainting/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) at this year‚Äôs CVPR conference.
Here‚Äôs what that means:

> We propose a method for converting a single RGB-D input image into a 3D photo, i.e., a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view.

Based on a single image (plus depth information), they can generate a 2.5-dimensional representation, realistically re-rendering the scene from slightly different perspectives from which it was originally taken.
Contrast that with recent work on neural radiance fields, which requires on the order of 20 - 50 images to work (see [DT #36](https://dynamicallytyped.com/issues/36-google-releases-tensorflow-quantum-software-2-0-at-plumerai-and-encoding-scenes-in-neural-networks-233576?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).

Shih et al.
set up [a website with some fancy demos](https://shihmengli.github.io/3D-Photo-Inpainting/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), which is definitely worth a look; see [these gifs on Twitter too](https://twitter.com/genekogan/status/1248650281249673217?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
One of the authors also works at Facebook, so I wonder if we‚Äôll one day see Instagram filters with this effect‚Äîor if it‚Äôll be a part of Facebook‚Äôs virtual reality ambitions.
Since the next generation of iPhones will likely have a depth sensor on the back too, I expect we‚Äôll see a lot of this 2.5D photography stuff in the coming years.

**Thanks for reading!**
As usual, you can let me know what you thought of today‚Äôs issue using the buttons below or by replying to this email.
If you‚Äôre new here, check out the [Dynamically Typed archives](https://dynamicallytyped.com/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) or subscribe below to get a new issues in your inbox every second Sunday.

**If you enjoyed this issue of Dynamically Typed, why not forward it to a friend?**
It‚Äôs by far the best thing you can do to help me grow this newsletter.
üåû