---
title: "#19: Microsoft&#x27;s $1B OpenAI investment, Lyft&#x27;s dataset, and what makes a peacock a peacock? "
date: 2019-08-04
number: 19
aliases:
  - https://dynamicallytyped.com/issues/19-microsoft-s-1b-openai-investment-lyft-s-dataset-and-what-makes-a-peacock-a-peacock-190545
---

Hey everyone, welcome to Dynamically Typed #19.
Today‚Äôs edition is a bit shorter than usual, but with good reason: I‚Äôm writing it early because the rest of this week I‚Äôll be super busy with Amsterdam Pride events!
üè≥Ô∏è‚Äçüåàüéâ

Anyway: the past fortnight‚Äôs biggest news was OpenAI‚Äôs $1 billion investment in OpenAI and the community‚Äôs reaction to their partnership.
Other stories include Ben Evan‚Äôs blog post about the productization of computer vision on the edge, Misa Ogura‚Äôs FlashTorch feature visualization toolkit, and Lyft‚Äôs open-sourcing of a self-driving car dataset.
Finally, National Geographic covered some interesting work being done in the AI + climate change space.
Let‚Äôs dive in.

## Productized Artificial Intelligence üîå

![OpenAI + Microsoft](https://s3.amazonaws.com/revue/items/images/004/850/973/mail/d104aee2ea4f68c6b6fe7ef3d204cca7.png?1564605294)

_OpenAI + Microsoft_

**Microsoft is investing $1 billion in OpenAI.**

To raise enough money to pay for the recruiting and computing needed to compete with the likes of DeepMind, much of the OpenAI nonprofit (and its employees) has been pivoted to become the for-profit OpenAI LP.
The community‚Äôs reaction, unsurprisingly, [has](https://twitter.com/soumithchintala/status/1153308199610511360?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) [been](https://twitter.com/tsimonite/status/1153340994986766336?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) [mixed](https://www.reddit.com//r/MachineLearning/comments/cgmptl/d_what_is_openai_i_dont_know_anymore/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter); especially OpenAI‚Äôs lofty marketing talk about the pre- and post-artificial general intelligence (AGI) periods of the company‚Äôs future raised some eyebrows.

This ‚Äúpre-AGI period‚Äù is also where the Microsoft investment and partnership comes in: they will become OpenAI‚Äôs ‚Äúexclusive cloud provider‚Äù and their main strategy for productizing the lab‚Äôs AI research.

> [We‚Äôll] be working hard together to further extend Microsoft Azure‚Äôs capabilities in large-scale AI systems.
> ‚Ä¶ [We] intend to license some of our pre-AGI technologies, with Microsoft becoming our preferred partner for commercializing them.

This is great for Microsoft‚Äôs AI cloud (and klout), but I‚Äôm not too sure what it says about the future of OpenAI.
Since they [won‚Äôt disclose the terms of the investment](https://twitter.com/gdb/status/1153305526026956800?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), it‚Äôs hard to know how much the company‚Äôs original mission‚Äî"the creation of beneficial AGI"‚Äîwill fall to the wayside to make room for productization.

(And if it does, is that necessarily a bad thing?
After all, productized AI is what this section of Dynamically Typed is all about.
I‚Äôd love to hear your thoughts, so use that reply button!)

Read more about OpenAI + Microsoft here:

* OpenAI press release: [Microsoft Invests In and Partners with OpenAI to Support Us Building Beneficial AGI](https://openai.com/blog/microsoft/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)
* Tom Simonite for Wired: [To Compete With Google, OpenAI Seeks Investors‚Äîand Profits](https://www.wired.com/story/compete-google-openai-seeks-investorsand-profits/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)

**Ben Evans of Andreessen Horowitz wrote a post about the potential of computer vision to touch almost everything.**
On the back of imaging sensors that have become ridiculously cheap in recent years (because of the efficiency of the smartphone supply chain), Evans argues that ‚Äúimaging plus ML‚Äù will power a lot more AI computing [on the edge](https://dynamicallytyped.com/issues/15-neural-avatars-ai-on-the-edge-and-apple-s-new-create-ml-app-180967?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter):

> The common thread across all of this is that vision replaces single-purpose inputs, and human mechanical Turks, with a general-purpose input.

I think that especially the former makes for an interesting thought experiment.
Let‚Äôs take a look at fire detection: in the past, we found out a house was on fire only once a human saw it; then, smoke alarms were invented that started doing that job (and saving lives) automatically; now, [imaging plus ML can ‚Äúsee‚Äù a fire](https://scholar.google.nl/scholar?as_sdt=0%2C5&btnG=&hl=en&q=fire%20detection%20cnn&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) on a camera feed‚Äîwithout the need of a single-purpose input.
Of course, smoke alarms are already cheap and widely available, so they probably won‚Äôt get replaced by AI cameras.

But what other specialized sensors can become more affordable or more ubiquitous if they can be replaced by a computer vision model?
And what things that a human can monitor now, for which we have no specialized sensors, can we start to track using AI-powered cameras?
That‚Äôs where cheap imaging plus ML will have a huge impact.
([Pervasive facial recognition](https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) and [image censorship](https://citizenlab.ca/2019/07/cant-picture-this-2-an-analysis-of-wechats-realtime-image-filtering-in-chats/?utm_campaign=6a63322d0c-EMAIL_CAMPAIGN_2019_07_22_04_56&utm_medium=email&utm_source=Benedict%27s%20newsletter&utm_term=0_4999ca107f-6a63322d0c-70536657), sadly, are obvious immoral examples that are already being put into production.)

As always, Evan‚Äôs post is worth a read: [Computers that can see](https://www.ben-evans.com/benedictevans/2019/7/19/computers-that-can-see?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

## Machine Learning Research üéõ

![FlashTorch visualizations of parts of images that have the most influence on ](https://s3.amazonaws.com/revue/items/images/004/850/887/mail/c020d8f99eb7a9520b3a0ddbfb1e9d2a.png?1564604403)

_FlashTorch visualizations of parts of images that have the most influence on _

**Misa Ogura wrote about FlashTorch, her open source feature visualization toolkit for neural networks in PyTorch.**
Her post first gives a nice history of feature visualization:

> Feature visualisation is an active area of research which aims to understand how neural networks perceive images, by exploring ways in which we can look ‚Äúthrough their eyes‚Äù.
> It has emerged and evolved in response to an increasing desire to make neural networks more interpretable to humans.

Ogura then covers class saliency maps, activation maximization, neuron arithmetic, and activation atlases (see [DT #9](https://dynamicallytyped.com/issues/9-openai-and-google-s-activation-atlases-a16z-s-ml-startup-investments-and-microsoft-s-ai-pipeline-163609?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).
Finally, she introduces FlashTorch, which implements saliency maps to show what parts of an image has the most influence on a classification, and dives into some some of its implementation details.
As someone who isn‚Äôt too deep into computer vision (yet ü§´), I learned a lot from this post.
More:

* Misa Ogura for Towards Data Science: [Uncovering what neural nets ‚Äúsee‚Äù with FlashTorch](https://towardsdatascience.com/feature-visualisation-in-pytorch-saliency-maps-a3f99d08f78a?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)
* GitHub repository: [MisaOgura/flashtorch](https://github.com/MisaOgura/flashtorch?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)
* Colab notebook: [Visualise image-specific class saliency with backpropagation](https://colab.research.google.com/github/MisaOgura/flashtorch/blob/master/examples/visualise_saliency_with_backprop_colab.ipynb?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)

**Lyft has open-sourced its autonomous driving dataset.**
It includes 55,000 human-labeled frames of 3D imagery, each taken from 7 cameras and 3 lidars:

> We‚Äôre thrilled to share a comprehensive, large-scale dataset featuring the raw sensor camera and LiDAR inputs as perceived by a fleet of multiple, high-end, autonomous vehicles in a bounded geographic area.
> This dataset also includes high quality, human-labelled 3D bounding boxes of traffic agents, an underlying HD spatial semantic map.

Data collection and labeling like this is immensely expensive, so I‚Äôm happily surprised to see Lyft take such a collaborative approach to the self-driving car problem.
I wonder how they convinced their investors to OK this.
Either way, check out Lyft‚Äôs page for the dataset on their Level 5 site, which includes some very cool interactive dataset samples: [Lyft Level 5 Dataset: Moving autonomous vehicles forward, together](https://level5.lyft.com/dataset/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

**Quick ML resource links ‚ö°Ô∏è** ([see all 32](https://www.notion.so/adab36fecaea4306880898f41dcb9cb3?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter&v=cb3a74562c914234ac171931dad6c2e4))

* SHapley Additive exPlanations (SHAP) is a unified approach to explain the output of any machine learning model using seven different methods. Link: [slundberg/shap](https://github.com/slundberg/shap?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)

## Artificial Intelligence for the Climate Crisis üåç

**National Geographic interviewed David Rolnick** , co-author of ‚ÄúTackling Climate Change with Machine Learning,‚Äù the 97-page vision paper that came out of the similarly named ICML workshop earlier this year (see [DT #16](https://dynamicallytyped.com/issues/16-finding-whales-with-ai-and-97-pages-of-ml-for-climate-change-183400?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).

> ‚ÄúIt‚Äôs surprising how many problems machine learning can meaningfully contribute to,‚Äù says Rolnick, who also helped organize the June workshop.

> The paper offers up 13 areas where machine learning can be deployed, including energy production, CO2 removal, education, solar geo-engineering, and finance.
> Within these fields, the possibilities include more energy-efficient buildings, creating new low-carbon materials, better monitoring of deforestation, and greener transportation.
> However, despite the potential, Rolnick points out that this is early days and AI can‚Äôt solve everything.

> ‚ÄúAI is not a silver bullet,‚Äù he says.

The article then goes deeper into three specific areas where machine learning researchers can help in mitigating the climate crisis:

* _Better climate predictions,_ including an ensemble learning method being developed by University of Colorado professor [Claire Monteleoni](https://www.colorado.edu/faculty/claire-monteleoni/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), who co-founded the [Climate Informatics workshop series](http://climateinformatics.org/?q=node%2F2&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (in 2011!) and [presented about climate change + AI at NeurIPS](http://research.microsoft.com/apps/video/?id=238888&utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (in 2014!).
* _Showing the effects of extreme weather_ , including work by Victor Schmidt who used GANs to visualize what effect extreme weather (intensified by climate change) can have on peoples‚Äô homes (see [DT #14](https://dynamicallytyped.com/issues/14-artificial-intelligence-for-medicine-and-the-climate-crisis-178557?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)).
* _Measuring where carbon is coming from,_ including Carbon Tracker, which uses computer vision to analyze where polution comes from using satellite photos.

For in-depth views into each of these projects, check out Jackie Snow‚Äôs article for National Geographic: [How artificial intelligence can tackle climate change](https://www.nationalgeographic.com/environment/2019/07/artificial-intelligence-climate-change/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

**Thanks for reading!**
As usual, you can let me know what you thought of today‚Äôs issue using the buttons below or by replying to this email.
If you‚Äôre new here, check out the [Dynamically Typed archives](https://dynamicallytyped.com/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) or subscribe below to get a new issues in your inbox every second Sunday.

**If you enjoyed this issue of Dynamically Typed, why not forward it to a friend?**
It‚Äôs by far the best thing you can do to help me grow this newsletter.
üè≥Ô∏è‚Äçüåà