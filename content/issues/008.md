---
title: "#8: Should OpenAI open-source their impressive new language model? "
date: 2019-03-03
number: 8
aliases:
  - /issues/8-should-openai-open-source-their-impressive-new-language-model-161119
---

Hey everyone, exciting news!
I‚Äôm now on the Pro plan of [Revue](https://www.getrevue.co?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), the service that powers this newsletter, which means that Dynamically Typed now has a snazzy new color scheme and a custom domain: [dynamicallytyped.com](http://dynamicallytyped.com?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

In today‚Äôs issue, I‚Äôm focusing on GPT-2, OpenAI‚Äôs impressive language model that has stirred quite some controversy in the research community over the past few weeks.
This deep-dive-on-one-topic format is new to the newsletter, so let me know if you like it!
In other news, Microsoft released an update to its HoloLens AR headset, Google‚Äôs DeepMind used deep learning to optimize a wind farm, and Lyft‚Äôs self-driving car team shared their map making principles.

## OpenAI's Controversial Language Model ü¶Ñ

![A sample from the GPT-2 language model. A human wrote the italicized prompt and a computer wrote the rest. (OpenAI)](https://s3.amazonaws.com/revue/items/images/004/305/820/mail/Screenshot_2019-03-02_at_09.57.52.png?1551520801)

_A sample from the GPT-2 language model. A human wrote the italicized prompt and a computer wrote the rest. (OpenAI)_

**Just over two weeks ago, researchers at**[ **OpenAI released GPT-2**](https://blog.openai.com/better-language-models/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#sample3) **, a language model trained in an unsupervised way on 40GB of text from the Internet.**
A language model is any algorithm that takes some words as input (‚Äúthe coffee is ‚Ä¶‚Äù) and tries to predict the most likely next word as output (‚Äú‚Ä¶ hot‚Äù); it is one of the most fundamental tools in Natural Language Processing (NLP) research.
OpenAI‚Äôs GPT-2 model can do some pretty cool stuff:

* Given a sentence or two of prompt text, GPT-2 can write a realistic-sounding news article‚Äîcomplete with made-up persons, quotes, and concepts‚Äîsuch as the one about the discovery of unicorns in South America above. [See more examples on OpenAI‚Äôs blog](https://blog.openai.com/better-language-models/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#sample1).
* Given a whole article as input, GPT-2 can write a summary if it‚Äôs prompted with ‚ÄúTL;DR:‚Äù ( _too long, didn‚Äôt read_ ), an abbreviation that people on the internet use as prefix to a short summary of their long post.
* Trained on pairs of multilingual sentences separated by an equals sign (like ‚ÄúEnglish sentence = Nederlandse zin‚Äù), GPT-2 can start to translate unseen sentences once it encounters an equals sign.

On the latter two tasks, GPT-2 doesn‚Äôt achieve state-of-the-art performance, but it‚Äôs still cool to see how the researchers hacked their language model into performing them at all.

**Now, for the controversy:** **OpenAI is not releasing the trained model, or the training data, to the world** , in a departure from their previous open source research approach.
From [their blog post](https://blog.openai.com/better-language-models/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#task1):

> Due to our concerns about malicious applications of the technology, we are not releasing the trained model.
> As an experiment in responsible disclosure, we are instead releasing a much [smaller model](https://github.com/openai/gpt-2?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) for researchers to experiment with, as well as a [technical paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).

It‚Äôs easy to imagine how GPT-2 could be used to automatically generate large volumes of realistic-sounding fake news or inflammatory text.
Indeed, [The Verge tested exactly that](https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-models-read-write-openai-gpt2?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter):

> [W]hen given a prompt like ‚ÄúJews control the media,‚Äù GPT-2 wrote: ‚ÄúThey control the universities.
> They control the world economy.
> How is this done?
> Through various mechanisms that are well documented in the book _The Jews in Power_ by Joseph Goebbels, the Hitler Youth and other key members of the Nazi Party.‚Äù

This is definitely a worrying sample, so I can appreciate OpenAI‚Äôs intentions in deciding not to release the model for anyone to do this.

**However, over the past few weeks, the research community has criticized this decision** on two fronts: (1) open-sourcing the model would make its capabilities more transparent to the research community and the world, and (2) the model can probably be replicated fairly easily by a state or organization that has sufficient resources.

Stanford‚Äôs Hugh Zhang has the best take in [his open letter published on The Gradient](https://thegradient.pub/openai-please-open-source-your-language-model/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter).
He compares GPT-2 to Photoshop, which also scared people when it first came out because it enables anyone who‚Äôs willing to put a few hours into learning the software to make realistic-looking fake images.
Zhang argues that _precisely because_ everyone knows that images can so easily be manipulated _(photoshopped)_ by anyone, ‚Äúsociety has emerged relatively unscathed‚Äù compared to eras in history when only those with enough power could manipulate images and most people believed that those images were real (like Stalin‚Äôs political propaganda).
An open source release would enable all sorts of art projects and stunts with GPT-2, so that knowledge of its capabilities would spread to much broader groups of the population that did not see the original announcement.
Keeping the model secret, however, means that only states and organizations can replicate and use it, possibly for nefarious purposes.

I agree with Zhang‚Äôs calls to open source GPT-2, and I hope that OpenAI reverses their decision.
What do you all think?

Read more about GPT-2 here:

* OpenAI blog: [Better Language Models and Their Implications](https://blog.openai.com/better-language-models/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter#sample1)
* James Vincent at The Verge: [OpenAI‚Äôs new multitalented AI writes, translates, and slanders](https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-models-read-write-openai-gpt2?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)
* Hugh Zhang of Stanford University, writing for The Gradient: [OpenAI: Please Open Source Your Language Model](https://thegradient.pub/openai-please-open-source-your-language-model/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)
* A related piece about digital signatures by Oren Etzioni at Harvard Business Review: [How Will We Prevent AI-Based Forgery?](https://hbr.org/2019/03/how-will-we-prevent-ai-based-forgery?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)

## Productized Artificial Intelligence üîå

![Microsoft's HoloLens 2. (Vjeran Pavic, The Verge)](https://s3.amazonaws.com/revue/items/images/004/307/248/mail/vpavic_190131_3213_0029_%281%29.jpg?1551564652)

_Microsoft's HoloLens 2. (Vjeran Pavic, The Verge)_

**Microsoft has unveiled the second version of its augmented reality headset: the HoloLens 2.**
The biggest upgrade is that the glasses‚Äô field of view (how much of what you see in front of you can be covered in holograms) has doubled in size, fixing the biggest complaint around the first version.
Another problem was the awkward ‚ÄúAir Tap‚Äù-based interaction model; I got to experience this once in a demo, and it definitely felt clumsy.
(Shooting ray guns at aliens bursting out of walls definitely made up for it though.) Microsoft has also fixed this issue, using an AI model that tracks up to 25 points in each of your hands and knows when you‚Äôre trying to grab and drag an object or press a button.
It can now also take the spatial map from the built-in Kinect sensor and semantically understand whether those thousands of 3D dots are part of a wall or a human.

Although the HoloLens 2 is exclusively aimed at businesses, I‚Äôm excited to see the technology trickle down to consumer products in the coming few years.
(Apple, for one, is a consumer-facing company that‚Äôs also investing heavily in AR.)

* Dieter Bohn got a demo of HoloLens 2 and wrote a great in-depth piece about it for The Verge: [Microsoft‚Äôs HoloLens 2: A $3,500 mixed reality headset for the factory, not the living room](https://www.theverge.com/2019/2/24/18235460/microsoft-hololens-2-price-specs-mixed-reality-ar-vr-business-work-features-mwc-2019?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)

**Google‚Äôs DeepMind used deep learning to optimize a wind farm, and made it 20% more valuable.**
Energy sources that can be scheduled are more valuable to an energy grid, which has always been a big drawback to renewables like wind energy because, well, the wind is rather unpredictable.
DeepMind trained a model to predict power output 36 hours in advance, so the wind farm could make hourly energy commitments a full day in advance.
This increased the value of the wind energy by 20% compared to a baseline of no time-based commitments.
I‚Äôd love to work on a project like this one day.

* Sims Witherspoon, Program Manager at DeepMind: [Machine learning can boost the value of wind energy](https://www.blog.google/technology/ai/machine-learning-can-boost-value-wind-energy/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)

## Machine Learning Technology üéõ

![The top Machine Learning projects on GitHub in 2018. (GitHub)](https://s3.amazonaws.com/revue/items/images/004/307/287/mail/51644284-84466380-1f24-11e9-8e96-72dc15458a41.png?1551565706)

_The top Machine Learning projects on GitHub in 2018. (GitHub)_

**GitHub published a report about the state of open source machine learning code,** featuring their analysis of contributions to public machine learning projects on the platform from January 1st to December 31st, 2018.
The most popular languages for such projects are Python, C++, and JavaScript, and common Python packages include numpy, scipy, pandas, and matplotlib.
The top ML projects on GitHub, pictured above, of course also include tensorflow and scikit-learn.
I don‚Äôt think any of these are particularly surprising, but it‚Äôs nice to know that the environments I‚Äôve been working in reflect the overall preferences of the community.

**Kumar Chellapilla at ride-hailing company Lyft shared the principles their Level 5 team is using to build maps for self-driving cars.**
Lots of companies in the automated vehicle (AV) space are currently trying to work out exactly what level of detail such maps need to get a network of self-driving taxis online; since no standard has emerged yet, it‚Äôs interesting to see what Lyft thinks is most important:

* _Mapping as pre-computation:_ besides the obvious static objects like roads and street signs that can be in a map, it can also include probability distributions for things like speed profiles and unmarked parking spaces; these partial / intermediate results can make the on-car computation easier.
* _Mapping to improve safety:_ these are things like speed profiles of how Lyft‚Äôs human drivers drive on a particular stretch of road.
* _Map as a unique sensor:_ the AV can fill in occluded parts of the its current 3D view of the world using map data.
* _Map as global shared state:_ the map is where cars can note down things like ‚Äúthere‚Äôs a construction site here!‚Äù for other cars.

Chellapilla‚Äôs post goes into how they implement this, including four of the ‚Äúlayers‚Äù of their maps, from geography to real-time knowledge.

* Read it on the Lyft Level 5 blog: [Rethinking Maps for Self-Driving](https://medium.com/@LyftLevel5/https-medium-com-lyftlevel5-rethinking-maps-for-self-driving-a147c24758d6?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter) (Medium)

## Cool Things ‚ú®

![Which of these is a real person, and which is a GAN-generated fake? (Which Face Is Real)](https://s3.amazonaws.com/revue/items/images/004/307/304/mail/Webp.net-resizeimage.png?1551566949)

_Which of these is a real person, and which is a GAN-generated fake? (Which Face Is Real)_

**I‚Äôm finishing today‚Äôs issue with Which Face Is Real, a site that shows you two faces and asks you to pick which one is real.**
It‚Äôs a project by the University of Washington‚Äôs [Calling Bullshit Project](http://callingbullshit.org/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter), which wants people to think critically about the information in front of them.
This is also an example of exactly what Hugh Zhang argued for regarding OpenAI‚Äôs GPT-2: the fact that the Generative Adversarial Network (GAN) that was used to generate these fake faces is open-source enables project like this to be created, which in turn spreads awareness of what the AI model is capable of.

* Give it a try here: [whichfaceisreal.com](http://www.whichfaceisreal.com/index.php?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)
* In a similar vein, This Person Does Not Exist shows a new computer-generated face on every refresh, powered by the same GAN: [thispersondoesnotexist.com](https://thispersondoesnotexist.com?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter)

**Thanks for reading!**
As always, let me know what you thought of this issue using the buttons below or by sending me a message.
(Especially if you have thoughts on the long-form first section!) If you‚Äôre new here, subscribe to get a new issue of Dynamically Typed in your inbox every second Sunday.